Task 1: Tuning with HyperBand for 180 small datasets - log

06/10/2021: Exploding that the hyperparameter return NaN when building a LSTM (without earlyStopping)
 - Check if the problem comes from the tensorflow for cpu?
 - Check if the problem comes from the parameters setup
 + Findout that: when the loss explodes and return NaN values, it stops the optimization.
 +>Solution: Adding the tf.keras.callbacks.TerminateOnNan()
 ++>Result: ./results/mse_07102021 and Log: ./logs/processing/mse_07102021
 ++>Change name: ./results/mse_07102021_ES and Log: ./logs/processing/mse_07102021_ES
 ++>Re-run with no early stopping: ./result/mse_08102021_noES and Log: ./logs/processing/mse_08102021_noES

13/10/2021: There still is a gap between the tuned hyperparameters and the default setup.
 +>Solution: Writing a custom metric based on professor's code.
 +>Addition: To increase the tuning speed, writing a custom callback function to stop the trial if the output accuracy below threshold.
 ++>Result: ./results/mse_13102021_ES and Log: ./logs/processing/mse_13102021_ES

19/10/2021: Re-run with no data scaling. 

11/11/2021: Re-organize files and directories, which are
+ ./results:
 ++> Move all the tuning results to folder 1_tuning
 ++> Move all the testing results for the child network to folder 2_rnn_plus
 ++> Generate folder 0_demo in case re-run for demo
+ ./logs:
 ++> Changing the ./processing and ./rerun folder like the ./results folder

-- Complete Task 1 with best result logs in: ----------------------------------
+ log: 
 ++> ./logs/processing/1_tuning/mse_13102021_ES (Data has been scaled)
 ++> ./logs/processing/1_tuning/mse_20102021_ES_NS (Data hasn't been scaled)
+ result:
 ++> ./results/1_tuning/mse_13102021_ES (Data has been scaled)
 ++> ./results/1_tuning/mse_20102021_ES (Data hasn't been scaled)
 -------------------------------------------------------------------------------
