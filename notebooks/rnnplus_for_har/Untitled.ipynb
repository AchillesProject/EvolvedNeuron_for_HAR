{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c261c12d-6f02-4a33-9caa-0e34cd1769aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2337ab0a134cb5aeb6a77c946d2835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 21:57:51 INFO: Downloading default packages for language: en (English)...\n",
      "2022-03-24 21:57:52 INFO: File exists: C:\\Users\\chaut\\stanza_resources\\en\\default.zip.\n",
      "2022-03-24 21:57:56 INFO: Finished downloading models and saved to C:\\Users\\chaut\\stanza_resources.\n",
      "2022-03-24 21:57:56 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-03-24 21:57:57 INFO: Use device: gpu\n",
      "2022-03-24 21:57:57 INFO: Loading: tokenize\n",
      "2022-03-24 21:58:00 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43570\n",
      "61153\n",
      "74289\n",
      "84918\n",
      "94428\n",
      "103285\n",
      "111220\n",
      "118449\n",
      "112204\n",
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, RobustScaler\n",
    "import sys, glob, os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from six.moves import zip\n",
    "import ipywidgets, stanza, json\n",
    "\n",
    "stanza.download('en')\n",
    "stanford_tokenizer = stanza.Pipeline('en',processors='tokenize', use_gpu= True)\n",
    "\n",
    "def customPTBTokenizer(texts, isLower=True, oov_token=None): \n",
    "    word_counts = OrderedDict()\n",
    "    chunk_size = 5000\n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        text = ' '.join(map(str, list(map(lambda x: x.lower(), texts[i:i+chunk_size]))))\n",
    "        seq = [word['text'] for doc in stanford_tokenizer(text).to_dict() for word in doc]\n",
    "        for w in seq:\n",
    "            word_counts[w] = 1 if w not in word_counts else word_counts[w] + 1\n",
    "        print(len(word_counts))\n",
    "    \n",
    "    wcounts = list(word_counts.items())\n",
    "    wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "    sorted_voc = [] if oov_token is None else [oov_token]\n",
    "    sorted_voc.extend(wc[0] for wc in wcounts)\n",
    "        \n",
    "    word_index = dict(zip(wcounts, list(range(1, len(wcounts) + 1))))\n",
    "    index_word = {c: w for w, c in word_index.items()}\n",
    "\n",
    "    return index_word\n",
    "    \n",
    "aclImdb_path = '../../../../Datasets/7_nlp/0_aclImdb_v1/csv'\n",
    "\n",
    "aclImdb_train = pd.read_csv(os.path.join(aclImdb_path, 'Train.csv'))\n",
    "aclImdb_val = pd.read_csv(os.path.join(aclImdb_path, 'Valid.csv')) \n",
    "\n",
    "#train_test split\n",
    "x_train, y_train = aclImdb_train['text'].values, aclImdb_train['label'].values\n",
    "x_val, y_val = aclImdb_val['text'].values, aclImdb_val['label'].values\n",
    "\n",
    "x_train_test = customPTBTokenizer(x_train)\n",
    "\n",
    "#Tokenize the sentences\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "#preparing vocabulary\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "\n",
    "#converting text into integer sequences\n",
    "x_train_seq  = tokenizer.texts_to_sequences(x_train) \n",
    "x_val_seq = tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding to prepare sequences of same length\n",
    "x_train_seq_pad  = pad_sequences(x_train_seq, maxlen=100)\n",
    "x_val_seq_pad = pad_sequences(x_val_seq, maxlen=100)\n",
    "\n",
    "size_of_vocabulary=len(tokenizer.word_index) + 1 #+1 for padding\n",
    "print(size_of_vocabulary)\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('../../../../Datasets/7_nlp/pretrained_models/glove.6B/glove.6B.300d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((size_of_vocabulary, 300))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dfc6612-948d-4632-9aeb-6620c3c62831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "NO_IN = 300\n",
    "NO_OUT = 1\n",
    "TIME_STEPS = 100\n",
    "PADDING_THRESHOLD = 50\n",
    "\n",
    "def prePadding(data, label, time_step=100, padding_threshold=60):\n",
    "    padded_label = [] \n",
    "    padded_data = []\n",
    "    for record_index, record in enumerate(data):\n",
    "        padded_record = []\n",
    "        if (len(record) % time_step) > padding_threshold:\n",
    "            padding_vector = [0] * (time_step - (len(record) % time_step))\n",
    "            padded_record = padding_vector + record\n",
    "        else:\n",
    "            padded_record = record[(len(record) % time_step):]\n",
    "        \n",
    "        for i in range(len(padded_record) // time_step):\n",
    "            padded_data.append(padded_record[i*time_step:(i+1)*time_step])\n",
    "            padded_label.append(label[record_index])\n",
    "            \n",
    "    return np.array(padded_data), np.array(padded_label).reshape(-1,1)\n",
    "\n",
    "x_train_seq_padded, y_train_padded = (prePadding(x_train_seq, y_train, time_step=TIME_STEPS, padding_threshold=PADDING_THRESHOLD))\n",
    "x_val_seq_padded,   y_val_padded   = (prePadding(x_val_seq,   y_val, time_step=TIME_STEPS,   padding_threshold=PADDING_THRESHOLD))\n",
    "\n",
    "with tf.device('cpu:0'):\n",
    "    embedding_layer = Embedding(size_of_vocabulary,NO_IN,weights=[embedding_matrix],input_length=TIME_STEPS,trainable=False, dtype=tf.float16)\n",
    "    embedding_layer.build(x_train_seq_padded.shape)\n",
    "    with open(fr\"{aclImdb_path}/aclImdb.ni={NO_IN}.no={NO_OUT}.ts={TIME_STEPS}.pt={PADDING_THRESHOLD}.train.csv\",'w') as csvfile:\n",
    "        np.savetxt(csvfile, np.array([[NO_IN, NO_OUT]]),fmt='%d', delimiter=\",\")\n",
    "    with open(fr\"{aclImdb_path}/aclImdb.ni={NO_IN}.no={NO_OUT}.ts={TIME_STEPS}.pt={PADDING_THRESHOLD}.train.csv\",'a') as csvfile:\n",
    "        np.savetxt(csvfile, tf.reshape(tf.concat([embedding_layer(x_train_seq_padded), tf.convert_to_tensor(np.expand_dims(np.tile(y_train_padded, TIME_STEPS), axis=2), dtype=tf.float16)], axis=2), [x_train_seq_padded.shape[0], -1]).numpy(), delimiter=\",\")\n",
    "   \n",
    "    with open(fr\"{aclImdb_path}/aclImdb.ni={NO_IN}.no={NO_OUT}.ts={TIME_STEPS}.pt={PADDING_THRESHOLD}.val.csv\",'w') as csvfile:\n",
    "        np.savetxt(csvfile, np.array([[NO_IN, NO_OUT]]),fmt='%d', delimiter=\",\")\n",
    "    with open(fr\"{aclImdb_path}/aclImdb.ni={NO_IN}.no={NO_OUT}.ts={TIME_STEPS}.pt={PADDING_THRESHOLD}.val.csv\",'a') as csvfile:\n",
    "        np.savetxt(csvfile, tf.reshape(tf.concat([embedding_layer(x_val_seq_padded), tf.convert_to_tensor(np.expand_dims(np.tile(y_val_padded, TIME_STEPS), axis=2), dtype=tf.float16)], axis=2), [x_val_seq_padded.shape[0], -1]).numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041d894-9609-4d45-954d-81d1b93fa368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "master_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
