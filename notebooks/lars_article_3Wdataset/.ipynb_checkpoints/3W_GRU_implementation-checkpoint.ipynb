{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ca1a31-4942-4d47-b616-015e4122abb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf:  2.11.0\n",
      "tb:  2.11.0\n",
      "C:\\Users\\chaut\\OneDrive - Heriot-Watt University\\HIOF_Master\\Master_Thesis\\NewLSTM\\Codes\\tf_implementations\\notebooks\\lars_article_3Wdataset\n",
      "{'batchSize': 4.0, 'numTrainingSteps': 320000.0, 'beta1': 0.970940445114, 'beta2': 0.996408592363, 'epsilon': 8.3258082e-05, 'decayDurationFactor': 0.92328699849, 'initialLearningRate': 0.000433155305, 'learningRateDecay': 1.2562e-08, 'glorotScaleFactor': 0.1, 'orthogonalScaleFactor': 0.1, 'noUnits': 128.0, 'testSize': 0.5}\n",
      "3w_64.train 3w_64.val\n",
      "\n",
      "Train and val shapes =  (1431, 527) (715, 527)\n",
      "Step 2: Separating values and labels.\n",
      "+ Training set:    (1431, 64, 8) (1431, 15) float64\n",
      "+ Validating set:  (715, 64, 8) (715, 15) float64\n",
      "+ Testing set:  (715, 64, 8) (715, 15) float64\n",
      "----------------------- GRU TRAINING ----------------------\n",
      "Epoch 1/223\n",
      "358/358 [==============================] - 10s 23ms/step - loss: 0.0516 - val_loss: 0.0727\n",
      "Epoch 2/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0439 - val_loss: 0.1295\n",
      "Epoch 3/223\n",
      "358/358 [==============================] - 8s 23ms/step - loss: 0.0421 - val_loss: 0.1347\n",
      "Epoch 4/223\n",
      "358/358 [==============================] - 8s 23ms/step - loss: 0.0410 - val_loss: 0.1204\n",
      "Epoch 5/223\n",
      "358/358 [==============================] - 8s 23ms/step - loss: 0.0395 - val_loss: 0.1219\n",
      "Epoch 6/223\n",
      "358/358 [==============================] - 8s 23ms/step - loss: 0.0386 - val_loss: 0.1220\n",
      "Epoch 7/223\n",
      "358/358 [==============================] - 8s 23ms/step - loss: 0.0366 - val_loss: 0.1050\n",
      "Epoch 8/223\n",
      "358/358 [==============================] - 8s 23ms/step - loss: 0.0352 - val_loss: 0.1102\n",
      "Epoch 9/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0340 - val_loss: 0.0830\n",
      "Epoch 10/223\n",
      "358/358 [==============================] - 10s 28ms/step - loss: 0.0328 - val_loss: 0.0772\n",
      "Epoch 11/223\n",
      "358/358 [==============================] - 8s 23ms/step - loss: 0.0316 - val_loss: 0.0810\n",
      "Epoch 12/223\n",
      "358/358 [==============================] - 10s 29ms/step - loss: 0.0311 - val_loss: 0.0866\n",
      "Epoch 13/223\n",
      "358/358 [==============================] - 11s 31ms/step - loss: 0.0313 - val_loss: 0.0847\n",
      "Epoch 14/223\n",
      "358/358 [==============================] - 10s 27ms/step - loss: 0.0325 - val_loss: 0.0807\n",
      "Epoch 15/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0319 - val_loss: 0.0893\n",
      "Epoch 16/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0295 - val_loss: 0.0888\n",
      "Epoch 17/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0292 - val_loss: 0.0967\n",
      "Epoch 18/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0287 - val_loss: 0.0976\n",
      "Epoch 19/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0289 - val_loss: 0.0955\n",
      "Epoch 20/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0282 - val_loss: 0.1022\n",
      "Epoch 21/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0268 - val_loss: 0.1101\n",
      "Epoch 22/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0285 - val_loss: 0.1086\n",
      "Epoch 23/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0270 - val_loss: 0.1078\n",
      "Epoch 24/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0267 - val_loss: 0.1076\n",
      "Epoch 25/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0259 - val_loss: 0.1108\n",
      "Epoch 26/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0270 - val_loss: 0.1074\n",
      "Epoch 27/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0257 - val_loss: 0.1084\n",
      "Epoch 28/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0258 - val_loss: 0.1138\n",
      "Epoch 29/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0259 - val_loss: 0.1127\n",
      "Epoch 30/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0262 - val_loss: 0.1085\n",
      "Epoch 31/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0286 - val_loss: 0.1053\n",
      "Epoch 32/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0252 - val_loss: 0.1124\n",
      "Epoch 33/223\n",
      "358/358 [==============================] - 8s 23ms/step - loss: 0.0247 - val_loss: 0.1129\n",
      "Epoch 34/223\n",
      "358/358 [==============================] - 8s 23ms/step - loss: 0.0258 - val_loss: 0.1121\n",
      "Epoch 35/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0273 - val_loss: 0.1165\n",
      "Epoch 36/223\n",
      "358/358 [==============================] - 10s 27ms/step - loss: 0.0266 - val_loss: 0.1048\n",
      "Epoch 37/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0261 - val_loss: 0.1107\n",
      "Epoch 38/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0256 - val_loss: 0.1053\n",
      "Epoch 39/223\n",
      "358/358 [==============================] - 8s 23ms/step - loss: 0.0259 - val_loss: 0.1030\n",
      "Epoch 40/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0261 - val_loss: 0.0953\n",
      "Epoch 41/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0246 - val_loss: 0.1012\n",
      "Epoch 42/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0232 - val_loss: 0.1029\n",
      "Epoch 43/223\n",
      "358/358 [==============================] - 8s 24ms/step - loss: 0.0234 - val_loss: 0.1040\n",
      "Epoch 44/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0224 - val_loss: 0.1063\n",
      "Epoch 45/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0217 - val_loss: 0.0977\n",
      "Epoch 46/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0215 - val_loss: 0.0991\n",
      "Epoch 47/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0231 - val_loss: 0.1074\n",
      "Epoch 48/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0219 - val_loss: 0.0987\n",
      "Epoch 49/223\n",
      "358/358 [==============================] - 10s 29ms/step - loss: 0.0231 - val_loss: 0.0923\n",
      "Epoch 50/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0246 - val_loss: 0.0951\n",
      "Epoch 51/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0253 - val_loss: 0.0925\n",
      "Epoch 52/223\n",
      "358/358 [==============================] - 8s 24ms/step - loss: 0.0228 - val_loss: 0.0967\n",
      "Epoch 53/223\n",
      "358/358 [==============================] - 8s 24ms/step - loss: 0.0216 - val_loss: 0.0984\n",
      "Epoch 54/223\n",
      "358/358 [==============================] - 8s 23ms/step - loss: 0.0224 - val_loss: 0.0925\n",
      "Epoch 55/223\n",
      "358/358 [==============================] - 8s 23ms/step - loss: 0.0215 - val_loss: 0.0985\n",
      "Epoch 56/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0222 - val_loss: 0.0958\n",
      "Epoch 57/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0215 - val_loss: 0.0951\n",
      "Epoch 58/223\n",
      "358/358 [==============================] - 9s 24ms/step - loss: 0.0212 - val_loss: 0.0930\n",
      "Epoch 59/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0216 - val_loss: 0.0928\n",
      "Epoch 60/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0201 - val_loss: 0.0930\n",
      "Epoch 61/223\n",
      "358/358 [==============================] - 10s 29ms/step - loss: 0.0204 - val_loss: 0.0953\n",
      "Epoch 62/223\n",
      "358/358 [==============================] - 10s 28ms/step - loss: 0.0205 - val_loss: 0.0950\n",
      "Epoch 63/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0200 - val_loss: 0.0949\n",
      "Epoch 64/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0209 - val_loss: 0.0882\n",
      "Epoch 65/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0209 - val_loss: 0.0921\n",
      "Epoch 66/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0209 - val_loss: 0.0889\n",
      "Epoch 67/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0217 - val_loss: 0.0930\n",
      "Epoch 68/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0205 - val_loss: 0.0887\n",
      "Epoch 69/223\n",
      "358/358 [==============================] - 11s 30ms/step - loss: 0.0228 - val_loss: 0.0924\n",
      "Epoch 70/223\n",
      "358/358 [==============================] - 10s 27ms/step - loss: 0.0213 - val_loss: 0.0961\n",
      "Epoch 71/223\n",
      "358/358 [==============================] - 10s 27ms/step - loss: 0.0195 - val_loss: 0.0937\n",
      "Epoch 72/223\n",
      "358/358 [==============================] - 10s 27ms/step - loss: 0.0195 - val_loss: 0.0970\n",
      "Epoch 73/223\n",
      "358/358 [==============================] - 11s 30ms/step - loss: 0.0197 - val_loss: 0.0953\n",
      "Epoch 74/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0206 - val_loss: 0.0978\n",
      "Epoch 75/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0201 - val_loss: 0.0940\n",
      "Epoch 76/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0197 - val_loss: 0.0932\n",
      "Epoch 77/223\n",
      "358/358 [==============================] - 10s 28ms/step - loss: 0.0198 - val_loss: 0.0945\n",
      "Epoch 78/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0203 - val_loss: 0.0895\n",
      "Epoch 79/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0195 - val_loss: 0.0909\n",
      "Epoch 80/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0192 - val_loss: 0.0948\n",
      "Epoch 81/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0188 - val_loss: 0.0912\n",
      "Epoch 82/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0188 - val_loss: 0.0925\n",
      "Epoch 83/223\n",
      "358/358 [==============================] - 9s 25ms/step - loss: 0.0183 - val_loss: 0.0903\n",
      "Epoch 84/223\n",
      "358/358 [==============================] - 9s 26ms/step - loss: 0.0190 - val_loss: 0.0919\n",
      "179/179 [==============================] - 1s 5ms/step\n",
      "179/179 [==============================] - 1s 5ms/step - loss: 0.0882\n",
      "GRU 3w_64.val val_performance = 0.08815360069274902\n",
      "GRU 3w_64.val val_accuracy = 0.41538000106811523\n",
      "179/179 [==============================] - 1s 5ms/step\n",
      "179/179 [==============================] - 1s 5ms/step - loss: 0.0376\n",
      "GRU 3w_64.test test_performance = 0.03761289641261101\n",
      "GRU 3w_64.test val_accuracy = 0.5146899819374084\n",
      "----------------------- LSTM TRAINING ----------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Dimension value must be integer or None or have an __index__ method, got value '128.0' with type '<class 'float'>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 122>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGRU \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtestFile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m val_accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(metric\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;241m5\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------- LSTM TRAINING ----------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_wLRS_wtCMF_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoIn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoOut\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m model_history \u001b[38;5;241m=\u001b[39m lstm_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    179\u001b[0m                     x_train, y_train,\n\u001b[0;32m    180\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatchSize\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, start_from_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)]\n\u001b[0;32m    188\u001b[0m                 )\n\u001b[0;32m    190\u001b[0m metric \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mCategoricalAccuracy()\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mlstm_wLRS_wtCMF_model\u001b[1;34m(noInput, noOutput, timestep)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlstm_wLRS_wtCMF_model\u001b[39m(noInput, noOutput, timestep):\n\u001b[0;32m     99\u001b[0m     model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[1;32m--> 100\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnoUnits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoInput\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtanh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurrent_activation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mrecurrent_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLSTM_layer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    103\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(noInput\u001b[38;5;241m+\u001b[39mnoOutput, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMLP_layer\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    104\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(noOutput))\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\volve\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py:573\u001b[0m, in \u001b[0;36mLSTM.__init__\u001b[1;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, return_sequences, return_state, go_backwards, stateful, time_major, unroll, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivity_regularizer \u001b[38;5;241m=\u001b[39m regularizers\u001b[38;5;241m.\u001b[39mget(activity_regularizer)\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_spec \u001b[38;5;241m=\u001b[39m [InputSpec(ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)]\n\u001b[1;32m--> 573\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_spec \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    574\u001b[0m     InputSpec(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, dim)) \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits)\n\u001b[0;32m    575\u001b[0m ]\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_could_use_gpu_kernel \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01min\u001b[39;00m (activations\u001b[38;5;241m.\u001b[39mtanh, tf\u001b[38;5;241m.\u001b[39mtanh)\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_activation \u001b[38;5;129;01min\u001b[39;00m (activations\u001b[38;5;241m.\u001b[39msigmoid, tf\u001b[38;5;241m.\u001b[39msigmoid)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mexecuting_eagerly_outside_functions()\n\u001b[0;32m    583\u001b[0m )\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_logical_devices(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;66;03m# Only show the message when there is GPU available, user will not\u001b[39;00m\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;66;03m# care about the cuDNN if there isn't any GPU.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\volve\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py:574\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivity_regularizer \u001b[38;5;241m=\u001b[39m regularizers\u001b[38;5;241m.\u001b[39mget(activity_regularizer)\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_spec \u001b[38;5;241m=\u001b[39m [InputSpec(ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)]\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_spec \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 574\u001b[0m     \u001b[43mInputSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits)\n\u001b[0;32m    575\u001b[0m ]\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_could_use_gpu_kernel \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01min\u001b[39;00m (activations\u001b[38;5;241m.\u001b[39mtanh, tf\u001b[38;5;241m.\u001b[39mtanh)\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_activation \u001b[38;5;129;01min\u001b[39;00m (activations\u001b[38;5;241m.\u001b[39msigmoid, tf\u001b[38;5;241m.\u001b[39msigmoid)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mexecuting_eagerly_outside_functions()\n\u001b[0;32m    583\u001b[0m )\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_logical_devices(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;66;03m# Only show the message when there is GPU available, user will not\u001b[39;00m\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;66;03m# care about the cuDNN if there isn't any GPU.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\volve\\lib\\site-packages\\keras\\engine\\input_spec.py:87\u001b[0m, in \u001b[0;36mInputSpec.__init__\u001b[1;34m(self, dtype, shape, ndim, max_ndim, min_ndim, axes, allow_last_axis_squeeze, name)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     77\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     85\u001b[0m ):\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorShape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m         shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:774\u001b[0m, in \u001b[0;36mTensorShape.__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;124;03m\"\"\"Creates a new TensorShape with the given dimensions.\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \n\u001b[0;32m    767\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;124;03m  TypeError: If dims cannot be converted to a list of dimensions.\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dims, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):  \u001b[38;5;66;03m# Most common case.\u001b[39;00m\n\u001b[1;32m--> 774\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mas_dimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    776\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:774\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;124;03m\"\"\"Creates a new TensorShape with the given dimensions.\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \n\u001b[0;32m    767\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;124;03m  TypeError: If dims cannot be converted to a list of dimensions.\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dims, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):  \u001b[38;5;66;03m# Most common case.\u001b[39;00m\n\u001b[1;32m--> 774\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mas_dimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dims)\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    776\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:738\u001b[0m, in \u001b[0;36mas_dimension\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    736\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 738\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:214\u001b[0m, in \u001b[0;36mDimension.__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    212\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(value\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__index__\u001b[39m())\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimension value must be integer or None or have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man __index__ method, got value \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{1!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    217\u001b[0m           value, \u001b[38;5;28mtype\u001b[39m(value))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    219\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimension \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m must be >= 0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value)\n",
      "\u001b[1;31mTypeError\u001b[0m: Dimension value must be integer or None or have an __index__ method, got value '128.0' with type '<class 'float'>'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "from tensorflow.keras.backend import eval\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, RobustScaler, StandardScaler\n",
    "\n",
    "import tensorboard\n",
    "import keras\n",
    "from keras.utils import tf_utils\n",
    "import pandas as pd #pd.plotting.register_matplotlib_converters\n",
    "import numpy as np\n",
    "import sys, os, math, time, datetime, re\n",
    "\n",
    "print(\"tf: \", tf.__version__)\n",
    "print(\"tb: \", tensorboard.__version__)\n",
    "print(os.getcwd())\n",
    "\n",
    "DTYPE = tf.float64\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(1)\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "\n",
    "snapshot = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Debugging with Tensorboard\n",
    "logdir=\"logs/fit/gru/\" + snapshot\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "ISMOORE_DATASETS = False\n",
    "FILESNUMBER = 1\n",
    "LSTMNUMBER  = 1\n",
    "\n",
    "with open(\"../../params/8sets_params/params_3W_1.txt\") as f:\n",
    "    hyperparams = dict([re.sub('['+' ,\\n'+']','',x.replace(' .', '')).split('=') for x in f][1:-1])\n",
    "hyperparams = dict([k, float(v)] for k, v in hyperparams.items())\n",
    "hyperparams['testSize'] = 0.500\n",
    "# hyperparams['noUnits'] = 81\n",
    "print(hyperparams)\n",
    "\n",
    "def seperateValues(data, noInput, noOutput, isMoore=True):\n",
    "    x_data, y_data = None, None\n",
    "    for i in range(data.shape[0]):\n",
    "        if isMoore:\n",
    "            x_data_i = data[i].reshape(-1, noInput+noOutput)\n",
    "            x_data_i, y_data_i = x_data_i[:, 0:noInput], x_data_i[-1, noInput:]\n",
    "        else:\n",
    "            x_data_i = data[i][:-noOutput].reshape(-1, noInput)\n",
    "            y_data_i = data[i][-noOutput:].reshape(-1, noOutput)\n",
    "        x_data = x_data_i[np.newaxis,:,:] if x_data is None else np.append(x_data, x_data_i[np.newaxis,:,:], axis=0)\n",
    "        y_data = y_data_i.reshape(1, -1)  if y_data is None else np.append(y_data, y_data_i.reshape(1, -1), axis=0)\n",
    "    return x_data, y_data\n",
    "\n",
    "class customLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, batchSize, initialLearningRate, learningRateDecay, \n",
    "                 decayDurationFactor, numTrainingSteps, name=None):\n",
    "        self.name                 = name\n",
    "        self.cell_dtype           = DTYPE\n",
    "        self.batchSize            = tf.constant(batchSize, dtype=self.cell_dtype, name=\"bz\")\n",
    "        self.initialLearningRate  = tf.constant(initialLearningRate, dtype=self.cell_dtype, name=\"lr0\") \n",
    "        self.learningRateDecay    = tf.constant(learningRateDecay, dtype=self.cell_dtype, name=\"alpha\")\n",
    "        self.decayDurationFactor  = tf.constant(decayDurationFactor, dtype=self.cell_dtype, name=\"beta\")\n",
    "        self.numTrainingSteps     = tf.constant(numTrainingSteps, dtype=self.cell_dtype, name=\"ortho\")\n",
    "        self.T                    = tf.constant(self.decayDurationFactor*(self.numTrainingSteps/self.batchSize), \n",
    "                                                dtype=self.cell_dtype, name=\"T\")\n",
    "        self.lr                   = tf.Variable(self.initialLearningRate, dtype=self.cell_dtype, name=\"lr\")\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        self.t = tf.cast(step, self.cell_dtype)\n",
    "        self.lr = tf.cond(self.t > self.T, \n",
    "           lambda: self.learningRateDecay * self.initialLearningRate,\n",
    "           lambda: self.initialLearningRate -(1.0-self.learningRateDecay)*self.initialLearningRate*self.t/self.T\n",
    "          )\n",
    "        return self.lr\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"name\":           self.name,\n",
    "            \"cell_dtype\":     self.cell_dtype,\n",
    "            \"batchSize\":      self.batchSize,\n",
    "            \"initial_lr\":     self.initialLearningRate,\n",
    "            \"decay_rate\":     self.learningRateDecay,\n",
    "            \"decay_duration\": self.decayDurationFactor,\n",
    "            \"training_step\":  self.numTrainingSteps,\n",
    "            \"curr_lr\":        self.lr\n",
    "        }\n",
    "\n",
    "def lstm_wLRS_wtCMF_model(noInput, noOutput, timestep):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(units=int(hyperparams['noUnits']), input_shape=[timestep, noInput],\n",
    "                   activation='tanh', recurrent_activation='sigmoid', unroll=False, use_bias=True,\n",
    "                   recurrent_dropout=0.0, return_sequences=False, name='LSTM_layer'))\n",
    "    model.add(tf.keras.layers.Dense(noInput+noOutput, activation='tanh', name='MLP_layer'))\n",
    "    model.add(tf.keras.layers.Dense(noOutput))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=customLRSchedule(hyperparams['batchSize'], hyperparams['initialLearningRate'], hyperparams['learningRateDecay'], hyperparams['decayDurationFactor'], hyperparams['numTrainingSteps']), \\\n",
    "                                    beta_1=hyperparams['beta1'], beta_2=hyperparams['beta2'], epsilon=hyperparams['epsilon'], amsgrad=False, name=\"tunedAdam_lstm\")\n",
    "    model.compile(optimizer=optimizer, loss = 'mse', run_eagerly=False)\n",
    "    return model\n",
    "\n",
    "def gru_wLRS_wtCMF_model(noInput, noOutput, timestep):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.GRU(units=int(hyperparams['noUnits']), input_shape=[timestep, noInput],\n",
    "                   activation='tanh', recurrent_activation='sigmoid', unroll=False, use_bias=True,\n",
    "                   recurrent_dropout=0.0, return_sequences=False, name='GRU_layer'))\n",
    "    model.add(tf.keras.layers.Dense(noInput+noOutput, activation='tanh', name='MLP_layer'))\n",
    "    model.add(tf.keras.layers.Dense(noOutput))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=customLRSchedule(hyperparams['batchSize'], hyperparams['initialLearningRate'], hyperparams['learningRateDecay'], hyperparams['decayDurationFactor'], hyperparams['numTrainingSteps']), \\\n",
    "                                    beta_1=hyperparams['beta1'], beta_2=hyperparams['beta2'], epsilon=hyperparams['epsilon'], amsgrad=False, name=\"tunedAdam_lstm\")\n",
    "    model.compile(optimizer=optimizer, loss = 'mse', run_eagerly=False)\n",
    "    return model\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    trainFile = f'3w_64.train'\n",
    "    valFile = f'3w_64.val'\n",
    "    testFile = f'3w_64.test'\n",
    "    print(trainFile, valFile)\n",
    "    df_train = np.array(pd.read_csv(os.path.join('.', trainFile ), skiprows=1))\n",
    "    df_val = np.array(pd.read_csv(os.path.join('.', valFile ), skiprows=1))\n",
    "    df_test = np.array(pd.read_csv(os.path.join('.', testFile ), skiprows=1))\n",
    "    \n",
    "    timestep = 64\n",
    "    noIn, noOut = 8, 15\n",
    "    print( \"\\nTrain and val shapes = \", df_train.shape, df_val.shape)\n",
    "    \n",
    "    print('Step 2: Separating values and labels.')\n",
    "    x_train, y_train = seperateValues(df_train, noIn, noOut, isMoore=ISMOORE_DATASETS)\n",
    "    x_val, y_val = seperateValues(df_val, noIn, noOut, isMoore=ISMOORE_DATASETS)\n",
    "    x_test, y_test = seperateValues(df_test, noIn, noOut, isMoore=ISMOORE_DATASETS)\n",
    "    scaler = StandardScaler()\n",
    "    x_train = (scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))).reshape(x_train.shape[0], timestep, noIn)\n",
    "    x_val = (scaler.fit_transform(x_val.reshape(x_val.shape[0], -1))).reshape(x_val.shape[0], timestep, noIn)\n",
    "    x_test = (scaler.fit_transform(x_test.reshape(x_val.shape[0], -1))).reshape(x_test.shape[0], timestep, noIn)\n",
    "\n",
    "    print(\"+ Training set:   \", x_train.shape, y_train.shape, x_train.dtype)\n",
    "    print(\"+ Validating set: \", x_val.shape, y_val.shape, x_val.dtype)\n",
    "    print(\"+ Testing set: \"   , x_test.shape, y_test.shape, x_test.dtype)\n",
    "    \n",
    "    gru_metric =tf.keras.metrics.CategoricalAccuracy()\n",
    "    print(\"----------------------- GRU TRAINING ----------------------\")\n",
    "    gru_model = gru_wLRS_wtCMF_model(noIn, noOut, timestep=timestep)\n",
    "    model_history = gru_model.fit(\n",
    "                        x_train, y_train,\n",
    "                        batch_size=int(hyperparams['batchSize']),\n",
    "                        verbose=1, # Suppress chatty output; use Tensorboard instead\n",
    "                        # epochs=10,\n",
    "                        epochs=int(hyperparams['numTrainingSteps']/(x_train.shape[0])),\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        shuffle=True,\n",
    "                        use_multiprocessing=False,\n",
    "                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor=f\"val_loss\", patience=20, mode=\"min\", start_from_epoch=20, restore_best_weights=True)]\n",
    "                    )\n",
    "    metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    gru_val_y_pred      = gru_model.predict(x_val, verbose=1, batch_size=int(hyperparams['batchSize']))\n",
    "    gru_val_performance = gru_model.evaluate(x_val, y_val, batch_size=int(hyperparams['batchSize']), verbose=1)\n",
    "    metric.update_state(y_val, gru_val_y_pred)\n",
    "    print(f\"GRU {valFile} val_performance = {gru_val_performance}\")\n",
    "    print(f\"GRU {valFile} val_accuracy = {round(metric.result().numpy(), 5)}\")\n",
    "\n",
    "\n",
    "    gru_test_y_pred      = gru_model.predict(x_test, verbose=1, batch_size=int(hyperparams['batchSize']))\n",
    "    gru_test_performance = gru_model.evaluate(x_test, y_test, batch_size=int(hyperparams['batchSize']), verbose=1)\n",
    "    metric.update_state(y_test, gru_test_y_pred)\n",
    "    print(f\"GRU {testFile} test_performance = {gru_test_performance}\")\n",
    "    print(f\"GRU {testFile} val_accuracy = {round(metric.result().numpy(), 5)}\")\n",
    "    \n",
    "    print(\"----------------------- LSTM TRAINING ----------------------\")\n",
    "    lstm_model = lstm_wLRS_wtCMF_model(noIn, noOut, timestep=timestep)\n",
    "    model_history = lstm_model.fit(\n",
    "                        x_train, y_train,\n",
    "                        batch_size=int(hyperparams['batchSize']),\n",
    "                        verbose=1, # Suppress chatty output; use Tensorboard instead\n",
    "                        # epochs=10,\n",
    "                        epochs=int(hyperparams['numTrainingSteps']/(x_train.shape[0])),\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        shuffle=True,\n",
    "                        use_multiprocessing=False,\n",
    "                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor=f\"val_loss\", patience=20, mode=\"min\", start_from_epoch=20, restore_best_weights=True)]\n",
    "                    )\n",
    "    \n",
    "    metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    lstm_val_y_pred      = lstm_model.predict(x_val, verbose=1, batch_size=int(hyperparams['batchSize']))\n",
    "    lstm_val_performance = lstm_model.evaluate(x_val, y_val, batch_size=int(hyperparams['batchSize']), verbose=1)\n",
    "    metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    metric.update_state(y_val, lstm_val_y_pred)\n",
    "    print(f\"{valFile} val_performance = {lstm_val_performance}\")\n",
    "    print(f\"{valFile} val_accuracy = {100*round(metric.result().numpy(), 5)}\")\n",
    "    \n",
    "    lstm_test_y_pred      = lstm_model.predict(x_test, verbose=1, batch_size=int(hyperparams['batchSize']))\n",
    "    lstm_test_performance = lstm_model.evaluate(x_test, y_test, batch_size=int(hyperparams['batchSize']), verbose=1)\n",
    "    metric.update_state(y_test, lstm_test_y_pred)\n",
    "    print(f\"{testFile} test_performance = {lstm_test_performance}\")\n",
    "    print(f\"{testFile} val_accuracy = {100*round(metric.result().numpy(), 5)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volve",
   "language": "python",
   "name": "volve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
