{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a296d2b1",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#29C5F6; color:white; padding:0px 10px; border-radius:5px;\">\n",
    "    <h1 style='margin:15px 15px; color:#000000; font-size:32px'><b>Data Generation (Processing)</b></h1>\n",
    "        <h2 style='margin:15px 15px; color:#000000; font-size:24px'>Addition, MNIST, PTB, and NTU RGB+D Problem</h2>\n",
    "</div>\n",
    "\n",
    "The work is under the **\"Master Thesis\"** by **Chau Tran** with the supervision from **Prof. Roland Olsson**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8138f95-5a02-4899-9e92-abebb9c63257",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#29C5F6; border-radius:5px; padding:0px 10px; \"><h3 style='margin:15px 15px'>2. Addition Problem</h3></div>\n",
    "\n",
    "Source: https://github.com/batzner/indrnn/blob/master/examples/addition_rnn.py\n",
    "\n",
    "Timesteps params: https://arxiv.org/abs/1803.04831\n",
    "\n",
    "BatchSize params: https://arxiv.org/pdf/1511.06464.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba6a7fb-fff1-4c05-8782-faf9c4179cb4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "from numpy import array\n",
    "\n",
    "batch_size_arr = [80, 50, 100, 180, 200]\n",
    "time_steps_arr = [100, 500, 1000, 5000, 10000, 15000]\n",
    "\n",
    "def generateAddingProblemData(batch_size, time_steps):\n",
    "    # Build the first sequence\n",
    "    add_values = np.random.rand(batch_size, time_steps)\n",
    "\n",
    "    # Build the second sequence with one 1 in each half and 0s otherwise\n",
    "    add_indices = np.zeros_like(add_values, dtype=int)\n",
    "    half = int(time_steps / 2)\n",
    "    for i in range(batch_size):\n",
    "        first_half = np.random.randint(half)\n",
    "        second_half = np.random.randint(half, time_steps)\n",
    "        add_indices[i, [first_half, second_half]] = 1\n",
    "\n",
    "    # Zip the values and indices in a third dimension:\n",
    "    # inputs has the shape (batch_size, time_steps, 2)\n",
    "    inputs = np.dstack((add_values, add_indices))\n",
    "    targets = np.sum(np.multiply(add_values, add_indices), axis=1)\n",
    "    data = np.column_stack((inputs.reshape(batch_size, time_steps*2), targets))\n",
    "    return inputs, targets, data\n",
    "\n",
    "for bs in batch_size_arr:\n",
    "    for ts in time_steps_arr:\n",
    "        _, _, addingproblemdata = (generateAddingProblemData(bs*2, ts))\n",
    "        with open(f\"../../Datasets/2_addingproblem/addingProblem.bs={bs}.ts={ts}.csv\",'w') as csvfile:\n",
    "            np.savetxt(csvfile, np.array([[2, 1]]),fmt='%d', delimiter=\",\")\n",
    "        with open(f\"../../Datasets/2_addingproblem/addingProblem.bs={bs}.ts={ts}.csv\",'a') as csvfile:\n",
    "            np.savetxt(csvfile, addingproblemdata, fmt='%.4f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523413e-5387-4cf6-98b2-9930cb0f6f7f",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#29C5F6; border-radius:5px; padding:0px 10px; \"><h3 style='margin:15px 15px'>3. MNIST Problem</h3></div>\n",
    "\n",
    "Source: https://github.com/batzner/indrnn/blob/8239a819100c40d5662f0d7440bfa7b539366b7f/examples/sequential_mnist.py#L258\n",
    "\n",
    "Hyperparams: https://arxiv.org/abs/1803.04831 and https://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ff0da-8977-40e1-b34b-acd0e74d7f7a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Data Dimension\n",
    "num_input = 28          # MNIST data input (image shape: 28x28)\n",
    "timesteps = 28          # Timesteps\n",
    "n_classes = 10          # Number of classes, one class per digit\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train_oh = np.zeros((y_train.shape[0], y_train.max()+1), dtype=np.float32)\n",
    "y_train_oh[np.arange(y_train.shape[0]), y_train] = 1\n",
    "y_test_oh = np.zeros((y_test.shape[0], y_test.max()+1), dtype=np.float32)\n",
    "y_test_oh[np.arange(y_test.shape[0]), y_test] = 1\n",
    "\n",
    "trainset = np.column_stack((x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2]),y_train_oh))\n",
    "testset = np.column_stack((x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2]),y_test_oh))\n",
    "mnist_problemdata = np.vstack((trainset, testset))\n",
    "display(trainset.shape)\n",
    "display(testset.shape)\n",
    "display(mnist_problemdata.shape)\n",
    "with open(f\"../../Datasets/3_mnist/mnist.ni={num_input}.no={n_classes}.ts={timesteps}.train={60000}.test={10000}.csv\",'w') as csvfile:\n",
    "    np.savetxt(csvfile, np.array([[num_input, n_classes]]),fmt='%d', delimiter=\",\")\n",
    "with open(f\"../../Datasets/3_mnist/mnist.ni={num_input}.no={n_classes}.ts={timesteps}.train={60000}.test={10000}.csv\",'a') as csvfile:\n",
    "    np.savetxt(csvfile, mnist_problemdata, fmt='%.4f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8690e-6d96-4251-8f5f-b94d7716da38",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#29C5F6; border-radius:5px; padding:0px 10px; \"><h3 style='margin:15px 15px'>4. Penn Treebank (PTB) Problem</h3></div>\n",
    "\n",
    "Source: \n",
    "* https://catalog.ldc.upenn.edu/LDC95T7 \n",
    "* https://github.com/Sunnydreamrain/IndRNN_pytorch/tree/master/PTB \n",
    "* https://gist.github.com/tmatha/905ae0c0d304119851d7432e5b359330\n",
    "\n",
    "Hyperparams: https://arxiv.org/abs/1803.04831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde44820-a48b-4a02-bd09-493dc0439958",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size=20\n",
    "seq_len=20\n",
    "clip_norm=5\n",
    "learning_rate=1.\n",
    "decay=0.5\n",
    "epochs=13\n",
    "epochs_no_decay=4\n",
    "\n",
    "ptbdataset_path = '../../Datasets/4_ptb/ptbdataset'\n",
    "\n",
    "#MIT License - Copyright (c) 2018 tmatha\n",
    "def features_labels(data_array,batch_size,seq_len,batch_first=True):\n",
    "    if len(data_array.shape) != 1:\n",
    "        raise ValueError('Expected 1-d data array, '\n",
    "                     'instead data array shape is {} '.format(data_array.shape))\n",
    "\n",
    "    def fold(used_array):\n",
    "        shaped_array=np.reshape(used_array,(batch_size,seq_len*steps),order='C')\n",
    "        if batch_first:\n",
    "            return np.concatenate(np.split(shaped_array,steps,axis=1),axis=0)\n",
    "        else:\n",
    "            return np.transpose(shaped_array)\n",
    "\n",
    "    steps=(data_array.shape[0]-1)//(batch_size*seq_len)\n",
    "    used=batch_size*seq_len*steps\n",
    "\n",
    "    features=fold(data_array[:used])\n",
    "    labels=fold(data_array[1:used+1])\n",
    "    Data=collections.namedtuple('Data',['features','labels'])\n",
    "    data_np = np.concatenate((features, labels), axis=1)\n",
    "    return Data(features=features,labels=labels),steps, data_np\n",
    "\n",
    "with open(f'{ptbdataset_path}/ptb.train.txt','r') as f1,open(f'{ptbdataset_path}/ptb.valid.txt','r') as f2,open(\n",
    "    f'{ptbdataset_path}/ptb.test.txt','r') as f3:\n",
    "    seq_train=f1.read().replace('\\n','<eos>').split(' ')\n",
    "    seq_valid=f2.read().replace('\\n','<eos>').split(' ')\n",
    "    seq_test=f3.read().replace('\\n','<eos>').split(' ')\n",
    "\n",
    "seq_train=list(filter(None,seq_train))\n",
    "seq_valid=list(filter(None,seq_valid))\n",
    "seq_test=list(filter(None,seq_test))\n",
    "\n",
    "vocab_train=set(seq_train)\n",
    "vocab_valid=set(seq_valid)\n",
    "vocab_test=set(seq_test)\n",
    "\n",
    "assert vocab_valid.issubset(vocab_train)\n",
    "assert vocab_test.issubset(vocab_train)\n",
    "print('vocab_train {}, vocab_valid {}, vocab_test {}'.format(\n",
    "    len(vocab_train),len(vocab_valid),len(vocab_test)))\n",
    "\n",
    "vocab_train=sorted(vocab_train)#must have deterministic ordering, so word2id dictionary is reproducible across invocations\n",
    "\n",
    "word2id={w:i for i,w in enumerate(vocab_train)} #id2word={i:w for i,w in enumerate(vocab_train)}\n",
    "\n",
    "ids_train=np.array([word2id[word] for word in seq_train],copy=False,order='C')\n",
    "ids_valid=np.array([word2id[word] for word in seq_valid],copy=False,order='C')\n",
    "ids_test=np.array([word2id[word] for word in seq_test],copy=False,order='C')\n",
    "\n",
    "data_train, steps_train, trainset_np = features_labels(ids_train, batch_size, seq_len, batch_first=False)\n",
    "data_valid, steps_valid, valset_np   = features_labels(ids_valid, batch_size, seq_len, batch_first=False)\n",
    "data_test,  steps_test, testset_np   = features_labels(ids_test,  batch_size, seq_len, batch_first=False)\n",
    "\n",
    "trainset_np = trainset_np / len(vocab_train)\n",
    "valset_np   = valset_np / len(vocab_valid)\n",
    "testset_np  = testset_np / len(vocab_test)\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(data_train).batch(seq_len, drop_remainder=True)\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices(data_valid).batch(seq_len, drop_remainder=True)\n",
    "dataset_test  = tf.data.Dataset.from_tensor_slices(data_test).batch(seq_len,  drop_remainder=True)\n",
    "\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_train}.train.csv\",'w') as csvfile:\n",
    "    np.savetxt(csvfile, np.array([[seq_len, seq_len]]),fmt='%d', delimiter=\",\")\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_train}.train.csv\",'a') as csvfile:\n",
    "    np.savetxt(csvfile, trainset_np, fmt='%.4f', delimiter=\",\")\n",
    "\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_valid}.val.csv\",'w') as csvfile:\n",
    "    np.savetxt(csvfile, np.array([[seq_len, seq_len]]),fmt='%d', delimiter=\",\")\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_valid}.val.csv\",'a') as csvfile:\n",
    "    np.savetxt(csvfile, valset_np, fmt='%.4f', delimiter=\",\")\n",
    "    \n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_test}.test.csv\",'w') as csvfile:\n",
    "    np.savetxt(csvfile, np.array([[seq_len, seq_len]]),fmt='%d', delimiter=\",\")\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_test}.test.csv\",'a') as csvfile:\n",
    "    np.savetxt(csvfile, testset_np, fmt='%.4f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34230baf-f674-4125-9925-893d774a15f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from doc3 import training_doc3\n",
    "# nltk.download('punkt')\n",
    "display(training_doc3)\n",
    "cleaned = re.sub(r'\\W+', ' ', training_doc3).lower()\n",
    "tokens = word_tokenize(cleaned)\n",
    "train_len = 4\n",
    "text_sequences = [tokens[i-train_len:i] for i in range(train_len, len(tokens))]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
    "vocabulary_size = len(tokenizer.word_counts)+1 #increased by 1 for the cause of padding\n",
    "n_sequences = np.array(sequences, dtype='int32')\n",
    "\n",
    "train_inputs, train_targets = n_sequences[:,:-1], n_sequences[:,-1]\n",
    "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)\n",
    "seq_len = train_inputs.shape[1]\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import LSTM\n",
    "# from keras.layers import Embedding\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))\n",
    "# model.add(LSTM(50,return_sequences=True))\n",
    "# model.add(LSTM(50))\n",
    "# model.add(Dense(50,activation='relu'))\n",
    "# model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "# # compiling the network\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.fit(train_inputs,train_targets,epochs=500,verbose=0)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "input_text = input().strip().lower()\n",
    "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "print(encoded_text, pad_encoded)\n",
    "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
    "    pred_word = tokenizer.index_word[i]\n",
    "    print(\"Next word suggestion:\",pred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c48a8-a967-4425-9cb0-71e1473c2ff4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "ptbdataset_path = '../../Datasets/4_ptb/ptbdataset'\n",
    "with open(f'{ptbdataset_path}/ptb.train.txt','r') as f1,open(f'{ptbdataset_path}/ptb.valid.txt','r') as f2,open(\n",
    "    f'{ptbdataset_path}/ptb.test.txt','r') as f3:\n",
    "    seq_train=f1.read().replace('\\n','')\n",
    "    seq_valid=f2.read().replace('\\n','')\n",
    "    seq_test=f3.read().replace('\\n','')\n",
    "\n",
    "# 1.Pre-processing data\n",
    "seq_train_cleaned = re.sub(r'\\W+', '', seq_train).lower()\n",
    "seq_train_cleaned = re.sub(r'\\W*\\b\\w{1,3}\\b', '', seq_train_cleaned)\n",
    "seq_train_tokens = word_tokenize(seq_train_cleaned)\n",
    "display(seq_train_tokens)\n",
    "train_len = 4\n",
    "text_seq_train = [seq_train_tokens[i-train_len:i] for i in range(train_len, len(tokens))]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_seq_train)\n",
    "seq_train = tokenizer.texts_to_sequences(text_seq_train)\n",
    "vocabulary_size = len(seq_train_tokens.word_counts)+1 #increased by 1 for the cause of padding\n",
    "n_sequences = np.array(sequences, dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e558c-3c2d-4bf7-bf4b-485453d9e519",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#29C5F6; border-radius:5px; padding:0px 10px; \"><h3 style='margin:15px 15px'>5. Skeleton based Action Recognition (NTU RGB+D) Problem</h3></div>\n",
    "\n",
    "Source: \n",
    "* https://github.com/zibeu/Independently-Recurrent-Neural-Network---IndRNN \n",
    "* https://github.com/Sunnydreamrain/IndRNN_pytorch/\n",
    "* https://gist.github.com/tmatha/905ae0c0d304119851d7432e5b359330\n",
    "\n",
    "Hyperparams: https://arxiv.org/abs/1803.04831\n",
    "\n",
    "Data Information: https://rose1.ntu.edu.sg/dataset/actionRecognition/\n",
    "\n",
    "NTU RGB+D is a large-scale dataset for RGB-D human action recognition. It involves 56,880 samples of 60 action classes collected from 40 subjects. The actions can be generally divided into three categories: 40 daily actions (e.g., drinking, eating, reading), nine health-related actions (e.g., sneezing, staggering, falling down), and 11 mutual actions (e.g., punching, kicking, hugging). These actions take place under 17 different scene conditions corresponding to 17 video sequences (i.e., S001–S017). The actions were captured using three cameras with different horizontal imaging viewpoints, namely, −45∘,0∘, and +45∘. Multi-modality information is provided for action characterization, including depth maps, 3D skeleton joint position, RGB frames, and infrared sequences. The performance evaluation is performed by a cross-subject test that split the 40 subjects into training and test groups, and by a cross-view test that employed one camera (+45∘) for testing, and the other two cameras for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ebd0e3-6f0a-4b88-bfb5-e04d07be8b6a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "SKELETON_DIR = '../../Datasets/5_nturgb+d/nturgb+d_skeletons'\n",
    "NPY_DIR = '../../Datasets/5_nturgb+d/nturgb+d_npy/'\n",
    "TRAIN_DS = '_train.csv'\n",
    "TEST_DS = '_test.csv'\n",
    "\n",
    "skeleton_files_mask = os.path.join(SKELETON_DIR, '*.skeleton')\n",
    "skeleton_files = glob.glob(skeleton_files_mask)\n",
    "\n",
    "\n",
    "max_frame_count = 300\n",
    "max_joints = 25\n",
    "\n",
    "full_ds = []\n",
    "\n",
    "#for idx, file_name in enumerate(skeleton_files[:568]):\n",
    "for idx, file_name in enumerate(skeleton_files):\n",
    "    # if idx%100 == 0:\n",
    "    #     print(idx)\n",
    "    basename = os.path.basename(file_name)\n",
    "    name = os.path.splitext(basename)[0]\n",
    "    label = name.split('A')[1]\n",
    "    with open(file_name) as f:\n",
    "        framecount = int(f.readline())\n",
    "\n",
    "        sequence_frames = []\n",
    "\n",
    "        for frame in range(framecount):\n",
    "            body_count = int(f.readline())\n",
    "            if body_count <= 0 or body_count>2:\n",
    "                # print('continue, no body')\n",
    "                break\n",
    "            joints_xyz = []\n",
    "            for body in range(body_count):\n",
    "                skeleton_info = f.readline()\n",
    "                joint_counts = int(f.readline()) #25\n",
    "                for joint in range(joint_counts):\n",
    "                    joint_info = f.readline()\n",
    "                    joint_info_array = joint_info.split()\n",
    "                    x, y, z = joint_info_array[:3]\n",
    "                    joint_info_xyz = np.array([float(x), float(y), float(z)])\n",
    "                    joints_xyz.append(joint_info_xyz)\n",
    "            pad_joints = max_joints - len(joints_xyz)\n",
    "            joints_xyz = np.array(joints_xyz)\n",
    "            joints_xyz = np.pad(joints_xyz, ((0, pad_joints), (0, 0)), mode='constant')\n",
    "            frame_xyz = np.stack(joints_xyz)\n",
    "            sequence_frames.append(frame_xyz)\n",
    "        if len(sequence_frames) > 0:\n",
    "            file_name = os.path.join(NPY_DIR, name+ '.npy')\n",
    "            sample = [name+'.npy', int(label)-1]\n",
    "            full_ds.append(sample)\n",
    "            np.save(file_name, np.array(sequence_frames))\n",
    "\n",
    "#train_ds = full_ds[:380]\n",
    "#test_ds = full_ds[380:]\n",
    "\n",
    "train_ds = full_ds[:40320]\n",
    "test_ds = full_ds[40320:]\n",
    "\n",
    "with open(os.path.join(NPY_DIR, TRAIN_DS), 'w') as train_ds_file:\n",
    "    writer = csv.writer(train_ds_file, lineterminator='\\n')\n",
    "    writer.writerows(train_ds)\n",
    "\n",
    "with open(os.path.join(NPY_DIR, TEST_DS), 'w') as test_ds_file:\n",
    "    writer = csv.writer(test_ds_file, lineterminator='\\n')\n",
    "    writer.writerows(test_ds)\n",
    "\n",
    "for idx, file_name in enumerate(glob.glob(os.path.join(NPY_DIR, '*.npy'))):\n",
    "    file = np.load(file_name)\n",
    "    frame_dict[file_name] = file.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8b6165-0a8f-4fef-b82f-561a6e23cd3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = np.array([])\n",
    "fileset = np.array([])\n",
    "seq_no_arr = [20, 30, 40, 50, 60]\n",
    "for seq_no in seq_no_arr:\n",
    "    for k, v in frame_dict.items():\n",
    "        if v > seq_no:\n",
    "            frameset = np.array([])\n",
    "            fileset = np.array([])\n",
    "            file = np.load(k)\n",
    "            for frame in range(int(v/seq_no)*seq_no):\n",
    "                if frameset.shape[0] == 0:\n",
    "                    frameset = file[frame].reshape(file.shape[1], 1, file.shape[2])\n",
    "                else:\n",
    "                    frameset = np.concatenate((frameset, file[frame].reshape(file.shape[1], 1, file.shape[2])), axis=1)\n",
    "                    if (frameset.shape[1]) % seq_no == 0:\n",
    "                        if fileset.shape[0] == 0:\n",
    "                            fileset = frameset.reshape(frameset.shape[0], frameset.shape[1]*frameset.shape[2])\n",
    "                        else:\n",
    "                            fileset = np.concatenate((fileset, frameset.reshape(frameset.shape[0], frameset.shape[1]*frameset.shape[2])), axis=0)\n",
    "                        frameset = np.array([])\n",
    "                    else:\n",
    "                        frameset = np.concatenate((frameset, file[frame].reshape(file.shape[1], 1, file.shape[2])), axis=1)\n",
    "            fileset = np.concatenate((fileset, np.tile(int(k.split('.')[-2].split('A')[-1]), fileset.shape[0]).reshape(-1, 1)), axis=1)\n",
    "            dataset = fileset if dataset.shape[0] == 0 else np.concatenate((dataset, fileset), axis=0)\n",
    "            display(dataset.shape)\n",
    "\n",
    "    with open(f\"../../Datasets/5_nturgb+d/nturgb+d.ni={3}.no={60}.ts={seq_no}.bs={50}.csv\",'w') as csvfile:\n",
    "        np.savetxt(csvfile, np.array([[3, 60]]),fmt='%d', delimiter=\",\")\n",
    "    with open(f\"../../Datasets/5_nturgb+d/nturgb+d.ni={3}.no={60}.ts={seq_no}.bs={50}.csv\",'a') as csvfile:\n",
    "        np.savetxt(csvfile, testset_np, fmt='%.4f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe48bf-2658-4043-b2ea-c29420664e58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## WIreless Sensor Data Mining (WISDM) - Human Activity Recognition Problem\n",
    "\n",
    "Source: \n",
    "* https://www.cis.fordham.edu/wisdm/dataset.php\n",
    "* https://github.com/AchillesProject/MLCourse2020/blob/main/Project2/HAR_WISDM_MLCourse_v0_DataExploration.ipynb (access required)\n",
    "\n",
    "Data Format: **[user],[activity],[timestamp],[x-acceleration],[y-accel],[z-accel]**\n",
    "\n",
    "Number of examples: 1,098,207\n",
    "\n",
    "Fields:\n",
    "* user: 1..36\n",
    "* activity: {Walking, Jogging, Sitting, Standing, Upstairs, Downstairs}\n",
    "* timestamp: nanoseconds\n",
    "* x-acceleration: floating-point values between -20 .. 20\n",
    "* y-accel: floating-point values between -20 .. 20\n",
    "* z-accel: floating-point values between -20 .. 20\n",
    "\n",
    "The acceleration in the x direction as measured by the android phone's accelerometer. A value of 10 = 1g = 9.81 m/s^2, and 0 = no acceleration. The acceleration recorded includes gravitational acceleration toward the center of the Earth, so that when the phone is at rest on a flat surface the vertical axis will register +-10.\n",
    "\n",
    "Data version 2 Information: https://archive.ics.uci.edu/ml/datasets/WISDM+Smartphone+and+Smartwatch+Activity+and+Biometrics+Dataset+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ae0d0-8823-4578-8c18-a9ffe1b605c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, RobustScaler\n",
    "import sys\n",
    "\n",
    "TIME_STEPS_arr = [90, 60, 50, 40]\n",
    "isSTEPS_arr = [True, False]\n",
    "SPLIT = 0.5\n",
    "\n",
    "def divideData_perUser(data, per=0.5):\n",
    "    train_df = pd.DataFrame()\n",
    "    val_df = pd.DataFrame()\n",
    "    X_df = pd.DataFrame()\n",
    "    for user in np.unique(data['user']):\n",
    "        dataPerUser = data[data['user']==user]\n",
    "        for tag in np.unique(dataPerUser['activity']):\n",
    "            dataPerActivity = dataPerUser[dataPerUser['activity']==tag]\n",
    "            n = len(dataPerActivity)\n",
    "            train_df = train_df.append(dataPerActivity[0:int(n*per)])\n",
    "            val_df = val_df.append(dataPerActivity[int(n*per):int(n)])\n",
    "            X_df = X_df.append(dataPerActivity)        \n",
    "    return X_df, train_df, val_df\n",
    "\n",
    "# Utils functions for segmenting windows\n",
    "def windows(data,window_size,step):\n",
    "    start = 0\n",
    "    while start< data.count():\n",
    "        yield int(start), int(start + window_size)\n",
    "        start+= step\n",
    "def segment_signal(data, window_size = 90, step=40,columns=[]):\n",
    "    segments = np.empty((0,window_size,len(columns)))\n",
    "    labels= np.empty((0))\n",
    "    for user in np.unique(data['user']):\n",
    "        userdata = data[(data.user == user)]\n",
    "        for tag in np.unique(userdata['activity']):\n",
    "            sub_class_data = userdata[(userdata.activity == tag)]\n",
    "            for (start, end) in windows(pd.Series(sub_class_data.index.values),window_size,step):\n",
    "                if end > sub_class_data.shape[0] - 1:\n",
    "                    end = sub_class_data.shape[0]\n",
    "                    true_length = end - start\n",
    "                    remaining_data_length = window_size - true_length\n",
    "                    start -= remaining_data_length\n",
    "                if (sub_class_data[start:end].isnull().values.any()):\n",
    "                    print(sub_class_data[start:end].isnull().sum())\n",
    "                if(sub_class_data[start:end].shape[0] == window_size):\n",
    "                    segments = np.vstack([segments,np.dstack([sub_class_data[column][start:end] for column in columns])])\n",
    "                    labels = np.append(labels, tag) \n",
    "    return segments, labels.reshape(-1, 1)\n",
    "\n",
    "wisdmdataset_path = '../../Datasets/6_wisdm/WISDM_ar_v1.1'\n",
    "COLUMNS = ['x_axis', 'y_axis', 'z_axis']\n",
    "\n",
    "rdf = pd.read_csv(f'{wisdmdataset_path}/WISDM_ar_v1.1_raw.txt', header=None, names=['user', 'activity', 'timestamp', 'x_axis', 'y_axis', 'z_axis'])\n",
    "rdf.z_axis.replace(regex=True, inplace=True, to_replace=r';', value=r'')\n",
    "rdf['x_axis'] = rdf.x_axis.astype(np.float64)\n",
    "rdf['y_axis'] = rdf.y_axis.astype(np.float64)\n",
    "rdf['z_axis'] = rdf.z_axis.astype(np.float64)\n",
    "rdf['timestamp'].apply(lambda x: float(x))\n",
    "rdf.dropna(axis=0, how='any', inplace=True)\n",
    "rdf['activity'] = LabelEncoder().fit(np.unique(rdf['activity'])).transform(rdf['activity'])\n",
    "\n",
    "X_df, train_df, val_df = divideData_perUser(rdf, SPLIT)\n",
    "\n",
    "for isSTEPS in isSTEPS_arr:\n",
    "    for TIME_STEPS in TIME_STEPS_arr:\n",
    "        STEP = int(round(TIME_STEPS/2,-1)) if isSTEPS else TIME_STEPS\n",
    "        print(TIME_STEPS, STEP)\n",
    "\n",
    "        X, y = segment_signal(X_df, window_size=TIME_STEPS, step=STEP,columns=COLUMNS)\n",
    "        X_train, y_train = segment_signal(train_df, window_size=TIME_STEPS, step=STEP,columns=COLUMNS)\n",
    "        X_val, y_val = segment_signal(val_df, window_size=TIME_STEPS, step=STEP,columns=COLUMNS)\n",
    "\n",
    "        y_train = OneHotEncoder().fit_transform(y_train).toarray()\n",
    "        y_val = OneHotEncoder().fit_transform(y_val).toarray()\n",
    "        y =  OneHotEncoder().fit_transform(y).toarray()\n",
    "\n",
    "        y_train = np.tile(y_train, TIME_STEPS).reshape((y_train.shape[0], TIME_STEPS, y_train.shape[1]))\n",
    "        y_val   = np.tile(y_val, TIME_STEPS).reshape((y_val.shape[0], TIME_STEPS, y_val.shape[1]))\n",
    "        y       = np.tile(y, TIME_STEPS).reshape((y.shape[0], TIME_STEPS, y.shape[1]))\n",
    "\n",
    "        df_train = np.concatenate((X_train, y_train), axis=2).reshape((X_train.shape[0], -1))\n",
    "        df_val = np.concatenate((X_val, y_val), axis=2).reshape((X_val.shape[0], -1))\n",
    "        df = np.concatenate((X,y), axis=2).reshape((X.shape[0], -1))\n",
    "        \n",
    "        print(X_train.shape, y_train.shape, df_train.shape)\n",
    "        print(X_val.shape, y_val.shape, df_val.shape)\n",
    "        print(X.shape, y.shape, df.shape)\n",
    "\n",
    "        with open(f\"{wisdmdataset_path}/../wisdm.ni={3}.no={6}.ts={TIME_STEPS}.os={STEP}.spit={0}.all.csv\",'w') as csvfile:\n",
    "            np.savetxt(csvfile, np.array([[3, 6]]),fmt='%d', delimiter=\",\")\n",
    "        with open(f\"{wisdmdataset_path}/../wisdm.ni={3}.no={6}.ts={TIME_STEPS}.os={STEP}.spit={0}.all.csv\",'a') as csvfile:\n",
    "            np.savetxt(csvfile, df, fmt='%.4f', delimiter=\",\")\n",
    "\n",
    "        with open(f\"{wisdmdataset_path}/../wisdm.ni={3}.no={6}.ts={TIME_STEPS}.os={STEP}.spit={int(SPLIT*100)}.train.csv\",'w') as csvfile:\n",
    "            np.savetxt(csvfile, np.array([[3, 6]]),fmt='%d', delimiter=\",\")\n",
    "        with open(f\"{wisdmdataset_path}/../wisdm.ni={3}.no={6}.ts={TIME_STEPS}.os={STEP}.spit={int(SPLIT*100)}.train.csv\",'a') as csvfile:\n",
    "            np.savetxt(csvfile, df_train, fmt='%.4f', delimiter=\",\")\n",
    "\n",
    "        with open(f\"{wisdmdataset_path}/../wisdm.ni={3}.no={6}.ts={TIME_STEPS}.os={STEP}.spit={int(SPLIT*100)}.val.csv\",'w') as csvfile:\n",
    "            np.savetxt(csvfile, np.array([[3, 6]]),fmt='%d', delimiter=\",\")\n",
    "        with open(f\"{wisdmdataset_path}/../wisdm.ni={3}.no={6}.ts={TIME_STEPS}.os={STEP}.spit={int(SPLIT*100)}.val.csv\",'a') as csvfile:\n",
    "            np.savetxt(csvfile, df_val, fmt='%.4f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ae469-d583-4b23-b436-676781251bb0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## WIreless Sensor Data Mining (WISDM) - Smartphone & Smartwatch Activity Problem\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/WISDM+Smartphone+and+Smartwatch+Activity+and+Biometrics+Dataset+\n",
    "\n",
    "Raw's format: **[subject-id],[activity],[timestamp],[x-accel],[y-accel],[z-accel]**\n",
    "\n",
    "Number of samples for non-hand-oriented activities (5 activities):\n",
    "* Phone acceleration: 1,338,067\n",
    "* Watch acceleration: 1,053,141\n",
    "* Phone gyroscope:    1,006,749\n",
    "* Watch gyroscope:    0,949,933\n",
    "\n",
    "Fields:\n",
    "* subject-id: 1600..1650 (51 participants)\n",
    "* activity: {Walking - A, Jogging - B, Stairs - C, Sitting - D, Standing - E}\n",
    "* timestamp: microsecond (Unix Time)\n",
    "* x-acceleration: floating-point (can be positive or negative)\n",
    "* y-accel: floating-point (can be positive or negative)\n",
    "* z-accel: floating-point (can be positive or negative)\n",
    "\n",
    "For the accelerometer sensor, the units are m/s2; while, for the gyroscope sensor, the units are radians/s. The force of gravity on Earth, which affects the accelerometer readings, is 9.8m/s2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33fd231f-c411-49d0-a7f8-bbadfabf2425",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 40\n",
      "(9561, 90, 6) (9561, 90, 5) (9561, 990)\n",
      "(9561, 90, 6) (9561, 90, 5) (9561, 990)\n",
      "(19064, 90, 6) (19064, 90, 5) (19064, 990)\n",
      "60 30\n",
      "(12742, 60, 6) (12742, 60, 5) (12742, 660)\n",
      "(12742, 60, 6) (12742, 60, 5) (12742, 660)\n",
      "(25311, 60, 6) (25311, 60, 5) (25311, 660)\n",
      "50 20\n",
      "(19061, 50, 6) (19061, 50, 5) (19061, 550)\n",
      "(19061, 50, 6) (19061, 50, 5) (19061, 550)\n",
      "(38022, 50, 6) (38022, 50, 5) (38022, 550)\n",
      "40 20\n",
      "(19064, 40, 6) (19064, 40, 5) (19064, 440)\n",
      "(19064, 40, 6) (19064, 40, 5) (19064, 440)\n",
      "(38025, 40, 6) (38025, 40, 5) (38025, 440)\n",
      "90 90\n",
      "(4287, 90, 6) (4287, 90, 5) (4287, 990)\n",
      "(4287, 90, 6) (4287, 90, 5) (4287, 990)\n",
      "(8517, 90, 6) (8517, 90, 5) (8517, 990)\n",
      "60 60\n",
      "(6380, 60, 6) (6380, 60, 5) (6380, 660)\n",
      "(6380, 60, 6) (6380, 60, 5) (6380, 660)\n",
      "(12744, 60, 6) (12744, 60, 5) (12744, 660)\n",
      "50 50\n",
      "(7671, 50, 6) (7671, 50, 5) (7671, 550)\n",
      "(7671, 50, 6) (7671, 50, 5) (7671, 550)\n",
      "(15300, 50, 6) (15300, 50, 5) (15300, 550)\n",
      "40 40\n",
      "(9565, 40, 6) (9565, 40, 5) (9565, 440)\n",
      "(9565, 40, 6) (9565, 40, 5) (9565, 440)\n",
      "(19088, 40, 6) (19088, 40, 5) (19088, 440)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, RobustScaler\n",
    "import sys, glob, os\n",
    "\n",
    "TIME_STEPS_arr = [90, 60, 50, 40]\n",
    "isSTEPS_arr = [True, False]\n",
    "SPLIT = 0.5\n",
    "COLUMNS = ['x_accel', 'y_accel', 'z_accel', 'x_gyro', 'y_gyro', 'z_gyro']\n",
    "activities_arr = ['A', 'B', 'C', 'D', 'E']\n",
    "def divideData_perUser(data, per=0.5):\n",
    "    train_df = pd.DataFrame()\n",
    "    val_df = pd.DataFrame()\n",
    "    X_df = pd.DataFrame()\n",
    "    for user in np.unique(data['user']):\n",
    "        dataPerUser = data[data['user']==user]\n",
    "        for tag in np.unique(dataPerUser['activity']):\n",
    "            # if tag in activities_arr:\n",
    "            dataPerActivity = dataPerUser[dataPerUser['activity']==tag]\n",
    "            n = len(dataPerActivity)\n",
    "            train_df = train_df.append(dataPerActivity[0:int(n*per)])\n",
    "            val_df = val_df.append(dataPerActivity[int(n*per):int(n)])\n",
    "            X_df = X_df.append(dataPerActivity)        \n",
    "    return X_df, train_df, val_df\n",
    "\n",
    "# Utils functions for segmenting windows\n",
    "def windows(data,window_size,step):\n",
    "    start = 0\n",
    "    while start< data.count():\n",
    "        yield int(start), int(start + window_size)\n",
    "        start+= step\n",
    "def segment_signal(data, window_size = 90, step=40,columns=[]):\n",
    "    segments = np.empty((0,window_size,len(columns)))\n",
    "    labels= np.empty((0))\n",
    "    for user in np.unique(data['user']):\n",
    "        userdata = data[(data.user == user)]\n",
    "        for tag in np.unique(userdata['activity']):\n",
    "            sub_class_data = userdata[(userdata.activity == tag)]\n",
    "            for (start, end) in windows(pd.Series(sub_class_data.index.values),window_size,step):\n",
    "                if end > sub_class_data.shape[0] - 1:\n",
    "                    end = sub_class_data.shape[0]\n",
    "                    true_length = end - start\n",
    "                    remaining_data_length = window_size - true_length\n",
    "                    start -= remaining_data_length\n",
    "                if (sub_class_data[start:end].isnull().values.any()):\n",
    "                    print(sub_class_data[start:end].isnull().sum())\n",
    "                if(sub_class_data[start:end].shape[0] == window_size):\n",
    "                    segments = np.vstack([segments,np.dstack([sub_class_data[column][start:end] for column in columns])])\n",
    "                    labels = np.append(labels, tag) \n",
    "    return segments, labels.reshape(-1, 1)\n",
    "\n",
    "wisdm_phone_path = '../../Datasets/6_wisdm/WISDM_ar_v2.0/wisdm-dataset/'\n",
    "wisdm_phone_accel_path = '../../Datasets/6_wisdm/WISDM_ar_v2.0/wisdm-dataset/raw/phone/accel'\n",
    "wisdm_phone_accel_files_mask = os.path.join(wisdm_phone_accel_path, '*.txt')\n",
    "wisdm_phone_accel_files = sorted(glob.glob(wisdm_phone_accel_files_mask))\n",
    "\n",
    "wisdm_phone_gyro_path = '../../Datasets/6_wisdm/WISDM_ar_v2.0/wisdm-dataset/raw/phone/gyro'\n",
    "wisdm_phone_gyro_files_mask = os.path.join(wisdm_phone_gyro_path, '*.txt')\n",
    "wisdm_phone_gyro_files = sorted(glob.glob(wisdm_phone_gyro_files_mask))\n",
    "\n",
    "wisdm_phone_data = pd.DataFrame()\n",
    "count = 0\n",
    "for accel_file, gyro_file in zip(wisdm_phone_accel_files, wisdm_phone_gyro_files):\n",
    "    accel_data = pd.read_csv(accel_file, header=None, names=['user', 'activity', 'timestamp', 'x_accel', 'y_accel', 'z_accel'], index_col=['user', 'activity', 'timestamp'])\n",
    "    accel_data.z_accel.replace(regex=True, inplace=True, to_replace=r';', value=r'')\n",
    "    accel_data = accel_data.loc[~accel_data.index.duplicated(keep='first')]\n",
    "    gyro_data = pd.read_csv(gyro_file, header=None, names=['user', 'activity', 'timestamp', 'x_gyro', 'y_gyro', 'z_gyro'], index_col=['user', 'activity', 'timestamp'])\n",
    "    gyro_data.z_gyro.replace(regex=True, inplace=True, to_replace=r';', value=r'')\n",
    "    gyro_data = gyro_data.loc[~gyro_data.index.duplicated(keep='first')]\n",
    "    user_data = pd.concat([accel_data, gyro_data], axis=1).dropna()\n",
    "    wisdm_phone_data = wisdm_phone_data.append(user_data)\n",
    "    \n",
    "wisdm_phone_data = wisdm_phone_data.reset_index()\n",
    "wisdm_phone_data['x_accel'] = wisdm_phone_data.x_accel.astype(np.float64)\n",
    "wisdm_phone_data['y_accel'] = wisdm_phone_data.y_accel.astype(np.float64)\n",
    "wisdm_phone_data['z_accel'] = wisdm_phone_data.z_accel.astype(np.float64)\n",
    "wisdm_phone_data['x_gyro'] = wisdm_phone_data.x_gyro.astype(np.float64)\n",
    "wisdm_phone_data['y_gyro'] = wisdm_phone_data.y_gyro.astype(np.float64)\n",
    "wisdm_phone_data['z_gyro'] = wisdm_phone_data.z_accel.astype(np.float64)\n",
    "wisdm_phone_data['timestamp'].apply(lambda x: float(x))\n",
    "wisdm_phone_data.dropna(axis=0, how='any', inplace=True)\n",
    "wisdm_phone_data = wisdm_phone_data[wisdm_phone_data.activity.isin(activities_arr) == True].reset_index()\n",
    "wisdm_phone_data['activity'] = LabelEncoder().fit(np.unique(wisdm_phone_data['activity'])).transform(wisdm_phone_data['activity'])\n",
    "\n",
    "X_df, train_df, val_df = divideData_perUser(wisdm_phone_data, SPLIT)\n",
    "\n",
    "for isSTEPS in isSTEPS_arr:\n",
    "    for TIME_STEPS in TIME_STEPS_arr:\n",
    "        STEP = int(round(TIME_STEPS/2,-1)) if isSTEPS else TIME_STEPS\n",
    "        print(TIME_STEPS, STEP)\n",
    "\n",
    "        X_train, y_train = segment_signal(train_df, window_size=TIME_STEPS, step=STEP,columns=COLUMNS)\n",
    "        X_val, y_val = segment_signal(val_df, window_size=TIME_STEPS, step=STEP,columns=COLUMNS)\n",
    "        X, y = segment_signal(X_df, window_size=TIME_STEPS, step=STEP,columns=COLUMNS)\n",
    "        \n",
    "        y_train = OneHotEncoder().fit_transform(y_train).toarray()\n",
    "        y_val = OneHotEncoder().fit_transform(y_val).toarray()\n",
    "        y =  OneHotEncoder().fit_transform(y).toarray()\n",
    "\n",
    "        y_train = np.tile(y_train, TIME_STEPS).reshape((y_train.shape[0], TIME_STEPS, y_train.shape[1]))\n",
    "        y_val   = np.tile(y_val, TIME_STEPS).reshape((y_val.shape[0], TIME_STEPS, y_val.shape[1]))\n",
    "        y       = np.tile(y, TIME_STEPS).reshape((y.shape[0], TIME_STEPS, y.shape[1]))\n",
    "\n",
    "        df_train = np.concatenate((X_train, y_train), axis=2).reshape((X_train.shape[0], -1))\n",
    "        df_val = np.concatenate((X_val, y_val), axis=2).reshape((X_val.shape[0], -1))\n",
    "        df = np.concatenate((X,y), axis=2).reshape((X.shape[0], -1))\n",
    "        \n",
    "        print(X_train.shape, y_train.shape, df_train.shape)\n",
    "        print(X_val.shape, y_val.shape, df_val.shape)\n",
    "        print(X.shape, y.shape, df.shape)\n",
    "\n",
    "        with open(f\"{wisdm_phone_path}/wisdm.ni={6}.no={5}.ts={TIME_STEPS}.os={STEP}.spit={0}.all.csv\",'w') as csvfile:\n",
    "            np.savetxt(csvfile, np.array([[6, 5]]),fmt='%d', delimiter=\",\")\n",
    "        with open(f\"{wisdm_phone_path}/wisdm.ni={6}.no={5}.ts={TIME_STEPS}.os={STEP}.spit={0}.all.csv\",'a') as csvfile:\n",
    "            np.savetxt(csvfile, df, fmt='%.4f', delimiter=\",\")\n",
    "\n",
    "        with open(f\"{wisdm_phone_path}/wisdm.ni={6}.no={5}.ts={TIME_STEPS}.os={STEP}.spit={int(SPLIT*100)}.train.csv\",'w') as csvfile:\n",
    "            np.savetxt(csvfile, np.array([[6, 5]]),fmt='%d', delimiter=\",\")\n",
    "        with open(f\"{wisdm_phone_path}/wisdm.ni={6}.no={5}.ts={TIME_STEPS}.os={STEP}.spit={int(SPLIT*100)}.train.csv\",'a') as csvfile:\n",
    "            np.savetxt(csvfile, df_train, fmt='%.4f', delimiter=\",\")\n",
    "\n",
    "        with open(f\"{wisdm_phone_path}/wisdm.ni={6}.no={5}.ts={TIME_STEPS}.os={STEP}.spit={int(SPLIT*100)}.val.csv\",'w') as csvfile:\n",
    "            np.savetxt(csvfile, np.array([[6, 5]]),fmt='%d', delimiter=\",\")\n",
    "        with open(f\"{wisdm_phone_path}/wisdm.ni={6}.no={5}.ts={TIME_STEPS}.os={STEP}.spit={int(SPLIT*100)}.val.csv\",'a') as csvfile:\n",
    "            np.savetxt(csvfile, df_val, fmt='%.4f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fa00f-0d94-49ef-995c-23a30369ca0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "master_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
