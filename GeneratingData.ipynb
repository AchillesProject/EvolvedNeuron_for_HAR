{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a296d2b1",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8138f95-5a02-4899-9e92-abebb9c63257",
   "metadata": {},
   "source": [
    "## Addition Problem\n",
    "Source: https://github.com/batzner/indrnn/blob/master/examples/addition_rnn.py\n",
    "\n",
    "Timesteps params: https://arxiv.org/abs/1803.04831\n",
    "\n",
    "BatchSize params: https://arxiv.org/pdf/1511.06464.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba6a7fb-fff1-4c05-8782-faf9c4179cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "from numpy import array\n",
    "\n",
    "batch_size_arr = [80, 50, 100, 180, 200]\n",
    "time_steps_arr = [100, 500, 1000, 5000, 10000, 15000]\n",
    "\n",
    "def generateAddingProblemData(batch_size, time_steps):\n",
    "    # Build the first sequence\n",
    "    add_values = np.random.rand(batch_size, time_steps)\n",
    "\n",
    "    # Build the second sequence with one 1 in each half and 0s otherwise\n",
    "    add_indices = np.zeros_like(add_values, dtype=int)\n",
    "    half = int(time_steps / 2)\n",
    "    for i in range(batch_size):\n",
    "        first_half = np.random.randint(half)\n",
    "        second_half = np.random.randint(half, time_steps)\n",
    "        add_indices[i, [first_half, second_half]] = 1\n",
    "\n",
    "    # Zip the values and indices in a third dimension:\n",
    "    # inputs has the shape (batch_size, time_steps, 2)\n",
    "    inputs = np.dstack((add_values, add_indices))\n",
    "    targets = np.sum(np.multiply(add_values, add_indices), axis=1)\n",
    "    data = np.column_stack((inputs.reshape(batch_size, time_steps*2), targets))\n",
    "    return inputs, targets, data\n",
    "\n",
    "for bs in batch_size_arr:\n",
    "    for ts in time_steps_arr:\n",
    "        _, _, addingproblemdata = (generateAddingProblemData(bs*2, ts))\n",
    "        with open(f\"../../Datasets/2_addingproblem/addingProblem.bs={bs}.ts={ts}.csv\",'w') as csvfile:\n",
    "            np.savetxt(csvfile, np.array([[2, 1]]),fmt='%d', delimiter=\",\")\n",
    "        with open(f\"../../Datasets/2_addingproblem/addingProblem.bs={bs}.ts={ts}.csv\",'a') as csvfile:\n",
    "            np.savetxt(csvfile, addingproblemdata, fmt='%.4f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523413e-5387-4cf6-98b2-9930cb0f6f7f",
   "metadata": {},
   "source": [
    "## MNIST Problem\n",
    "\n",
    "Source: https://github.com/batzner/indrnn/blob/8239a819100c40d5662f0d7440bfa7b539366b7f/examples/sequential_mnist.py#L258\n",
    "\n",
    "Hyperparams: https://arxiv.org/abs/1803.04831 and https://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "918ff0da-8977-40e1-b34b-acd0e74d7f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 794)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 794)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(70000, 794)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Data Dimension\n",
    "num_input = 28          # MNIST data input (image shape: 28x28)\n",
    "timesteps = 28          # Timesteps\n",
    "n_classes = 10          # Number of classes, one class per digit\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train_oh = np.zeros((y_train.shape[0], y_train.max()+1), dtype=np.float32)\n",
    "y_train_oh[np.arange(y_train.shape[0]), y_train] = 1\n",
    "y_test_oh = np.zeros((y_test.shape[0], y_test.max()+1), dtype=np.float32)\n",
    "y_test_oh[np.arange(y_test.shape[0]), y_test] = 1\n",
    "\n",
    "trainset = np.column_stack((x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2]),y_train_oh))\n",
    "testset = np.column_stack((x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2]),y_test_oh))\n",
    "mnist_problemdata = np.vstack((trainset, testset))\n",
    "display(trainset.shape)\n",
    "display(testset.shape)\n",
    "display(mnist_problemdata.shape)\n",
    "with open(f\"../../Datasets/3_mnist/mnist.ni={num_input}.no={n_classes}.ts={timesteps}.train={60000}.test={10000}.csv\",'w') as csvfile:\n",
    "    np.savetxt(csvfile, np.array([[num_input, n_classes]]),fmt='%d', delimiter=\",\")\n",
    "with open(f\"../../Datasets/3_mnist/mnist.ni={num_input}.no={n_classes}.ts={timesteps}.train={60000}.test={10000}.csv\",'a') as csvfile:\n",
    "    np.savetxt(csvfile, mnist_problemdata, fmt='%.4f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8690e-6d96-4251-8f5f-b94d7716da38",
   "metadata": {},
   "source": [
    "## Penn Treebank (PTB) Problem\n",
    "\n",
    "Source: \n",
    "* https://catalog.ldc.upenn.edu/LDC95T7 \n",
    "* https://github.com/Sunnydreamrain/IndRNN_pytorch/tree/master/PTB \n",
    "* https://gist.github.com/tmatha/905ae0c0d304119851d7432e5b359330\n",
    "\n",
    "Hyperparams: https://arxiv.org/abs/1803.04831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cde44820-a48b-4a02-bd09-493dc0439958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_train 10000, vocab_valid 6022, vocab_test 6049\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size=20\n",
    "seq_len=20\n",
    "clip_norm=5\n",
    "learning_rate=1.\n",
    "decay=0.5\n",
    "epochs=13\n",
    "epochs_no_decay=4\n",
    "\n",
    "ptbdataset_path = '../../Datasets/4_ptb/ptbdataset'\n",
    "\n",
    "#MIT License - Copyright (c) 2018 tmatha\n",
    "def features_labels(data_array,batch_size,seq_len,batch_first=True):\n",
    "    if len(data_array.shape) != 1:\n",
    "        raise ValueError('Expected 1-d data array, '\n",
    "                     'instead data array shape is {} '.format(data_array.shape))\n",
    "\n",
    "    def fold(used_array):\n",
    "        shaped_array=np.reshape(used_array,(batch_size,seq_len*steps),order='C')\n",
    "        if batch_first:\n",
    "            return np.concatenate(np.split(shaped_array,steps,axis=1),axis=0)\n",
    "        else:\n",
    "            return np.transpose(shaped_array)\n",
    "\n",
    "    steps=(data_array.shape[0]-1)//(batch_size*seq_len)\n",
    "    used=batch_size*seq_len*steps\n",
    "\n",
    "    features=fold(data_array[:used])\n",
    "    labels=fold(data_array[1:used+1])\n",
    "    Data=collections.namedtuple('Data',['features','labels'])\n",
    "    data_np = np.concatenate((features, labels), axis=1)\n",
    "    return Data(features=features,labels=labels),steps, data_np\n",
    "\n",
    "with open(f'{ptbdataset_path}/ptb.train.txt','r') as f1,open(f'{ptbdataset_path}/ptb.valid.txt','r') as f2,open(\n",
    "    f'{ptbdataset_path}/ptb.test.txt','r') as f3:\n",
    "    seq_train=f1.read().replace('\\n','<eos>').split(' ')\n",
    "    seq_valid=f2.read().replace('\\n','<eos>').split(' ')\n",
    "    seq_test=f3.read().replace('\\n','<eos>').split(' ')\n",
    "\n",
    "seq_train=list(filter(None,seq_train))\n",
    "seq_valid=list(filter(None,seq_valid))\n",
    "seq_test=list(filter(None,seq_test))\n",
    "\n",
    "vocab_train=set(seq_train)\n",
    "vocab_valid=set(seq_valid)\n",
    "vocab_test=set(seq_test)\n",
    "\n",
    "assert vocab_valid.issubset(vocab_train)\n",
    "assert vocab_test.issubset(vocab_train)\n",
    "print('vocab_train {}, vocab_valid {}, vocab_test {}'.format(\n",
    "    len(vocab_train),len(vocab_valid),len(vocab_test)))\n",
    "\n",
    "vocab_train=sorted(vocab_train)#must have deterministic ordering, so word2id dictionary is reproducible across invocations\n",
    "\n",
    "word2id={w:i for i,w in enumerate(vocab_train)}\n",
    "id2word={i:w for i,w in enumerate(vocab_train)}\n",
    "\n",
    "ids_train=np.array([word2id[word] for word in seq_train],copy=False,order='C')\n",
    "ids_valid=np.array([word2id[word] for word in seq_valid],copy=False,order='C')\n",
    "ids_test=np.array([word2id[word] for word in seq_test],copy=False,order='C')\n",
    "\n",
    "data_train, steps_train, trainset_np = features_labels(ids_train, batch_size, seq_len, batch_first=False)\n",
    "data_valid, steps_valid, valset_np   = features_labels(ids_valid, batch_size, seq_len, batch_first=False)\n",
    "data_test,  steps_test, testset_np   = features_labels(ids_test,  batch_size, seq_len, batch_first=False)\n",
    "\n",
    "trainset_np = trainset_np / len(vocab_train)\n",
    "valset_np   = valset_np / len(vocab_valid)\n",
    "testset_np  = testset_np / len(vocab_test)\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(data_train).batch(seq_len, drop_remainder=True)\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices(data_valid).batch(seq_len, drop_remainder=True)\n",
    "dataset_test  = tf.data.Dataset.from_tensor_slices(data_test).batch(seq_len,  drop_remainder=True)\n",
    "\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_train}.train.csv\",'w') as csvfile:\n",
    "    np.savetxt(csvfile, np.array([[seq_len, seq_len]]),fmt='%d', delimiter=\",\")\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_train}.train.csv\",'a') as csvfile:\n",
    "    np.savetxt(csvfile, trainset_np, fmt='%.4f', delimiter=\",\")\n",
    "\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_valid}.val.csv\",'w') as csvfile:\n",
    "    np.savetxt(csvfile, np.array([[seq_len, seq_len]]),fmt='%d', delimiter=\",\")\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_valid}.val.csv\",'a') as csvfile:\n",
    "    np.savetxt(csvfile, valset_np, fmt='%.4f', delimiter=\",\")\n",
    "    \n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_test}.test.csv\",'w') as csvfile:\n",
    "    np.savetxt(csvfile, np.array([[seq_len, seq_len]]),fmt='%d', delimiter=\",\")\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_test}.test.csv\",'a') as csvfile:\n",
    "    np.savetxt(csvfile, testset_np, fmt='%.4f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e558c-3c2d-4bf7-bf4b-485453d9e519",
   "metadata": {},
   "source": [
    "## Skeleton based Action Recognition (NTU RGB+D) Problem\n",
    "\n",
    "Source: \n",
    "* https://github.com/zibeu/Independently-Recurrent-Neural-Network---IndRNN \n",
    "* https://github.com/Sunnydreamrain/IndRNN_pytorch/\n",
    "* https://gist.github.com/tmatha/905ae0c0d304119851d7432e5b359330\n",
    "\n",
    "Hyperparams: https://arxiv.org/abs/1803.04831\n",
    "\n",
    "Data Information: https://rose1.ntu.edu.sg/dataset/actionRecognition/\n",
    "\n",
    "NTU RGB+D is a large-scale dataset for RGB-D human action recognition. It involves 56,880 samples of 60 action classes collected from 40 subjects. The actions can be generally divided into three categories: 40 daily actions (e.g., drinking, eating, reading), nine health-related actions (e.g., sneezing, staggering, falling down), and 11 mutual actions (e.g., punching, kicking, hugging). These actions take place under 17 different scene conditions corresponding to 17 video sequences (i.e., S001–S017). The actions were captured using three cameras with different horizontal imaging viewpoints, namely, −45∘,0∘, and +45∘. Multi-modality information is provided for action characterization, including depth maps, 3D skeleton joint position, RGB frames, and infrared sequences. The performance evaluation is performed by a cross-subject test that split the 40 subjects into training and test groups, and by a cross-view test that employed one camera (+45∘) for testing, and the other two cameras for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ebd0e3-6f0a-4b88-bfb5-e04d07be8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "continue, no body\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "800\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "900\n",
      "1000\n",
      "continue, no body\n",
      "continue, no body\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "1400\n",
      "continue, no body\n",
      "1500\n",
      "continue, no body\n",
      "continue, no body\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "continue, no body\n",
      "continue, no body\n",
      "2000\n",
      "continue, no body\n",
      "2100\n",
      "2200\n",
      "continue, no body\n",
      "2300\n",
      "continue, no body\n",
      "2400\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "2500\n",
      "continue, no body\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "continue, no body\n",
      "3800\n",
      "continue, no body\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "continue, no body\n",
      "4200\n",
      "4300\n",
      "continue, no body\n",
      "4400\n",
      "continue, no body\n",
      "continue, no body\n",
      "4500\n",
      "4600\n",
      "continue, no body\n",
      "4700\n",
      "continue, no body\n",
      "4800\n",
      "4900\n",
      "continue, no body\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "continue, no body\n",
      "continue, no body\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "continue, no body\n",
      "5700\n",
      "continue, no body\n",
      "continue, no body\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "continue, no body\n",
      "continue, no body\n",
      "6100\n",
      "continue, no body\n",
      "6200\n",
      "continue, no body\n",
      "continue, no body\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "continue, no body\n",
      "continue, no body\n",
      "7200\n",
      "7300\n",
      "continue, no body\n",
      "continue, no body\n",
      "7400\n",
      "continue, no body\n",
      "continue, no body\n",
      "7500\n",
      "continue, no body\n",
      "7600\n",
      "continue, no body\n",
      "7700\n",
      "continue, no body\n",
      "7800\n",
      "7900\n",
      "continue, no body\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "continue, no body\n",
      "8300\n",
      "8400\n",
      "continue, no body\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "continue, no body\n",
      "8900\n",
      "continue, no body\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "continue, no body\n",
      "continue, no body\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "continue, no body\n",
      "continue, no body\n",
      "9900\n",
      "10000\n",
      "continue, no body\n",
      "continue, no body\n",
      "10100\n",
      "10200\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "10300\n",
      "continue, no body\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "continue, no body\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "continue, no body\n",
      "11300\n",
      "continue, no body\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "11800\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "11900\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "12000\n",
      "12100\n",
      "continue, no body\n",
      "12200\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "12300\n",
      "12400\n",
      "continue, no body\n",
      "12500\n",
      "continue, no body\n",
      "continue, no body\n",
      "12600\n",
      "continue, no body\n",
      "12700\n",
      "continue, no body\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "continue, no body\n",
      "13200\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "continue, no body\n",
      "13600\n",
      "continue, no body\n",
      "continue, no body\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "continue, no body\n",
      "14000\n",
      "14100\n",
      "continue, no body\n",
      "14200\n",
      "14300\n",
      "continue, no body\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "continue, no body\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "15100\n",
      "continue, no body\n",
      "15200\n",
      "continue, no body\n",
      "continue, no body\n",
      "15300\n",
      "continue, no body\n",
      "15400\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "continue, no body\n",
      "15800\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "15900\n",
      "continue, no body\n",
      "continue, no body\n",
      "16000\n",
      "16100\n",
      "continue, no body\n",
      "16200\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "16300\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "16800\n",
      "continue, no body\n",
      "continue, no body\n",
      "16900\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "17000\n",
      "continue, no body\n",
      "17100\n",
      "17200\n",
      "continue, no body\n",
      "17300\n",
      "17400\n",
      "continue, no body\n",
      "continue, no body\n",
      "17500\n",
      "17600\n",
      "continue, no body\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "18000\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "18100\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "continue, no body\n",
      "18500\n",
      "18600\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "18700\n",
      "18800\n",
      "continue, no body\n",
      "continue, no body\n",
      "18900\n",
      "19000\n",
      "continue, no body\n",
      "continue, no body\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "19500\n",
      "19600\n",
      "continue, no body\n",
      "19700\n",
      "19800\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "19900\n",
      "continue, no body\n",
      "continue, no body\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "continue, no body\n",
      "continue, no body\n",
      "20300\n",
      "continue, no body\n",
      "20400\n",
      "continue, no body\n",
      "20500\n",
      "continue, no body\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "continue, no body\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "21500\n",
      "continue, no body\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "continue, no body\n",
      "continue, no body\n",
      "21900\n",
      "continue, no body\n",
      "continue, no body\n",
      "22000\n",
      "22100\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "22200\n",
      "22300\n",
      "continue, no body\n",
      "22400\n",
      "continue, no body\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "continue, no body\n",
      "23100\n",
      "continue, no body\n",
      "23200\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "23300\n",
      "continue, no body\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "continue, no body\n",
      "continue, no body\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "continue, no body\n",
      "24000\n",
      "continue, no body\n",
      "24100\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "24200\n",
      "continue, no body\n",
      "continue, no body\n",
      "24300\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "24400\n",
      "continue, no body\n",
      "24500\n",
      "continue, no body\n",
      "24600\n",
      "continue, no body\n",
      "continue, no body\n",
      "24700\n",
      "24800\n",
      "continue, no body\n",
      "continue, no body\n",
      "24900\n",
      "continue, no body\n",
      "25000\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "25100\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "25200\n",
      "25300\n",
      "continue, no body\n",
      "continue, no body\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "26000\n",
      "continue, no body\n",
      "continue, no body\n",
      "26100\n",
      "continue, no body\n",
      "26200\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n",
      "26300\n",
      "continue, no body\n",
      "continue, no body\n",
      "continue, no body\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "SKELETON_DIR = '../../Datasets/5_nturgb+d/nturgb+d_skeletons'\n",
    "NPY_DIR = '../../Datasets/5_nturgb+d/nturgp+d_npy/'\n",
    "TRAIN_DS = '_train.csv'\n",
    "TEST_DS = '_test.csv'\n",
    "\n",
    "skeleton_files_mask = os.path.join(SKELETON_DIR, '*.skeleton')\n",
    "skeleton_files = glob.glob(skeleton_files_mask)\n",
    "\n",
    "\n",
    "max_frame_count = 300\n",
    "max_joints = 50\n",
    "\n",
    "full_ds = []\n",
    "\n",
    "#for idx, file_name in enumerate(skeleton_files[:568]):\n",
    "for idx, file_name in enumerate(skeleton_files):\n",
    "    if idx%100 == 0:\n",
    "        print(idx)\n",
    "    basename = os.path.basename(file_name)\n",
    "    name = os.path.splitext(basename)[0]\n",
    "    label = name.split('A')[1]\n",
    "    with open(file_name) as f:\n",
    "        framecount = int(f.readline())\n",
    "\n",
    "        sequence_frames = []\n",
    "\n",
    "        for frame in range(framecount):\n",
    "            body_count = int(f.readline())\n",
    "            if body_count <= 0 or body_count>2:\n",
    "                print('continue, no body')\n",
    "                break\n",
    "            joints_xyz = []\n",
    "            for body in range(body_count):\n",
    "                skeleton_info = f.readline()\n",
    "                joint_counts = int(f.readline()) #25\n",
    "                for joint in range(joint_counts):\n",
    "                    joint_info = f.readline()\n",
    "                    joint_info_array = joint_info.split()\n",
    "                    x, y, z = joint_info_array[:3]\n",
    "                    joint_info_xyz = np.array([float(x), float(y), float(z)])\n",
    "                    joints_xyz.append(joint_info_xyz)\n",
    "            pad_joints = max_joints - len(joints_xyz)\n",
    "            joints_xyz = np.array(joints_xyz)\n",
    "            joints_xyz = np.pad(joints_xyz, ((0, pad_joints), (0, 0)), mode='constant')\n",
    "            frame_xyz = np.stack(joints_xyz)\n",
    "            sequence_frames.append(frame_xyz)\n",
    "        if len(sequence_frames) > 0:\n",
    "            file_name = os.path.join(NPY_DIR, name+ '.npy')\n",
    "            sample = [name+'.npy', int(label)-1]\n",
    "            full_ds.append(sample)\n",
    "            np.save(file_name, np.array(sequence_frames))\n",
    "\n",
    "#train_ds = full_ds[:380]\n",
    "#test_ds = full_ds[380:]\n",
    "\n",
    "train_ds = full_ds[:40320]\n",
    "test_ds = full_ds[40320:]\n",
    "\n",
    "with open(os.path.join(NPY_DIR, TRAIN_DS), 'w') as train_ds_file:\n",
    "    writer = csv.writer(train_ds_file, lineterminator='\\n')\n",
    "    writer.writerows(train_ds)\n",
    "\n",
    "with open(os.path.join(NPY_DIR, TEST_DS), 'w') as test_ds_file:\n",
    "    writer = csv.writer(test_ds_file, lineterminator='\\n')\n",
    "    writer.writerows(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c8cfb-5f3b-4fa1-975d-cc677c156eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "master_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
