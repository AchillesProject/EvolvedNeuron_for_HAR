{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a296d2b1",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8138f95-5a02-4899-9e92-abebb9c63257",
   "metadata": {},
   "source": [
    "## Addition Problem\n",
    "Source: https://github.com/batzner/indrnn/blob/master/examples/addition_rnn.py\n",
    "\n",
    "Timesteps params: https://arxiv.org/abs/1803.04831\n",
    "\n",
    "BatchSize params: https://arxiv.org/pdf/1511.06464.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba6a7fb-fff1-4c05-8782-faf9c4179cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "from numpy import array\n",
    "\n",
    "batch_size_arr = [80, 50, 100, 180, 200]\n",
    "time_steps_arr = [100, 500, 1000, 5000, 10000, 15000]\n",
    "\n",
    "def generateAddingProblemData(batch_size, time_steps):\n",
    "    # Build the first sequence\n",
    "    add_values = np.random.rand(batch_size, time_steps)\n",
    "\n",
    "    # Build the second sequence with one 1 in each half and 0s otherwise\n",
    "    add_indices = np.zeros_like(add_values, dtype=int)\n",
    "    half = int(time_steps / 2)\n",
    "    for i in range(batch_size):\n",
    "        first_half = np.random.randint(half)\n",
    "        second_half = np.random.randint(half, time_steps)\n",
    "        add_indices[i, [first_half, second_half]] = 1\n",
    "\n",
    "    # Zip the values and indices in a third dimension:\n",
    "    # inputs has the shape (batch_size, time_steps, 2)\n",
    "    inputs = np.dstack((add_values, add_indices))\n",
    "    targets = np.sum(np.multiply(add_values, add_indices), axis=1)\n",
    "    data = np.column_stack((inputs.reshape(batch_size, time_steps*2), targets))\n",
    "    return inputs, targets, data\n",
    "\n",
    "for bs in batch_size_arr:\n",
    "    for ts in time_steps_arr:\n",
    "        _, _, addingproblemdata = (generateAddingProblemData(bs*2, ts))\n",
    "        with open(f\"../../Datasets/2_addingproblem/addingProblem.bs={bs}.ts={ts}.csv\",'w') as csvfile:\n",
    "            np.savetxt(csvfile, np.array([[2, 1]]),fmt='%d', delimiter=\",\")\n",
    "        with open(f\"../../Datasets/2_addingproblem/addingProblem.bs={bs}.ts={ts}.csv\",'a') as csvfile:\n",
    "            np.savetxt(csvfile, addingproblemdata, fmt='%.4f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523413e-5387-4cf6-98b2-9930cb0f6f7f",
   "metadata": {},
   "source": [
    "## MNIST Problem\n",
    "\n",
    "Source: https://github.com/batzner/indrnn/blob/8239a819100c40d5662f0d7440bfa7b539366b7f/examples/sequential_mnist.py#L258\n",
    "\n",
    "Hyperparams: https://arxiv.org/abs/1803.04831 and https://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "918ff0da-8977-40e1-b34b-acd0e74d7f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 794)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 794)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(70000, 794)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Data Dimension\n",
    "num_input = 28          # MNIST data input (image shape: 28x28)\n",
    "timesteps = 28          # Timesteps\n",
    "n_classes = 10          # Number of classes, one class per digit\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train_oh = np.zeros((y_train.shape[0], y_train.max()+1), dtype=np.float32)\n",
    "y_train_oh[np.arange(y_train.shape[0]), y_train] = 1\n",
    "y_test_oh = np.zeros((y_test.shape[0], y_test.max()+1), dtype=np.float32)\n",
    "y_test_oh[np.arange(y_test.shape[0]), y_test] = 1\n",
    "\n",
    "trainset = np.column_stack((x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2]),y_train_oh))\n",
    "testset = np.column_stack((x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2]),y_test_oh))\n",
    "mnist_problemdata = np.vstack((trainset, testset))\n",
    "display(trainset.shape)\n",
    "display(testset.shape)\n",
    "display(mnist_problemdata.shape)\n",
    "with open(f\"../../Datasets/3_mnist/mnist.ni={num_input}.no={n_classes}.ts={timesteps}.train={60000}.test={10000}.csv\",'w') as csvfile:\n",
    "    np.savetxt(csvfile, np.array([[num_input, n_classes]]),fmt='%d', delimiter=\",\")\n",
    "with open(f\"../../Datasets/3_mnist/mnist.ni={num_input}.no={n_classes}.ts={timesteps}.train={60000}.test={10000}.csv\",'a') as csvfile:\n",
    "    np.savetxt(csvfile, mnist_problemdata, fmt='%.4f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8690e-6d96-4251-8f5f-b94d7716da38",
   "metadata": {},
   "source": [
    "## Penn Treebank (PTB) Problem\n",
    "\n",
    "Source: \n",
    "* https://catalog.ldc.upenn.edu/LDC95T7 \n",
    "* https://github.com/Sunnydreamrain/IndRNN_pytorch/tree/master/PTB \n",
    "* https://gist.github.com/tmatha/905ae0c0d304119851d7432e5b359330\n",
    "\n",
    "Hyperparams: https://arxiv.org/abs/1803.04831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cde44820-a48b-4a02-bd09-493dc0439958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_train 10000, vocab_valid 6022, vocab_test 6049\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size=20\n",
    "seq_len=20\n",
    "clip_norm=5\n",
    "learning_rate=1.\n",
    "decay=0.5\n",
    "epochs=13\n",
    "epochs_no_decay=4\n",
    "\n",
    "ptbdataset_path = '../../Datasets/4_ptb/ptbdataset'\n",
    "\n",
    "#MIT License - Copyright (c) 2018 tmatha\n",
    "def features_labels(data_array,batch_size,seq_len,batch_first=True):\n",
    "    if len(data_array.shape) != 1:\n",
    "        raise ValueError('Expected 1-d data array, '\n",
    "                     'instead data array shape is {} '.format(data_array.shape))\n",
    "\n",
    "    def fold(used_array):\n",
    "        shaped_array=np.reshape(used_array,(batch_size,seq_len*steps),order='C')\n",
    "        if batch_first:\n",
    "            return np.concatenate(np.split(shaped_array,steps,axis=1),axis=0)\n",
    "        else:\n",
    "            return np.transpose(shaped_array)\n",
    "\n",
    "    steps=(data_array.shape[0]-1)//(batch_size*seq_len)\n",
    "    used=batch_size*seq_len*steps\n",
    "\n",
    "    features=fold(data_array[:used])\n",
    "    labels=fold(data_array[1:used+1])\n",
    "    Data=collections.namedtuple('Data',['features','labels'])\n",
    "    data_np = np.concatenate((features, labels), axis=1)\n",
    "    return Data(features=features,labels=labels),steps, data_np\n",
    "\n",
    "with open(f'{ptbdataset_path}/ptb.train.txt','r') as f1,open(f'{ptbdataset_path}/ptb.valid.txt','r') as f2,open(\n",
    "    f'{ptbdataset_path}/ptb.test.txt','r') as f3:\n",
    "    seq_train=f1.read().replace('\\n','<eos>').split(' ')\n",
    "    seq_valid=f2.read().replace('\\n','<eos>').split(' ')\n",
    "    seq_test=f3.read().replace('\\n','<eos>').split(' ')\n",
    "\n",
    "seq_train=list(filter(None,seq_train))\n",
    "seq_valid=list(filter(None,seq_valid))\n",
    "seq_test=list(filter(None,seq_test))\n",
    "\n",
    "vocab_train=set(seq_train)\n",
    "vocab_valid=set(seq_valid)\n",
    "vocab_test=set(seq_test)\n",
    "\n",
    "assert vocab_valid.issubset(vocab_train)\n",
    "assert vocab_test.issubset(vocab_train)\n",
    "print('vocab_train {}, vocab_valid {}, vocab_test {}'.format(\n",
    "    len(vocab_train),len(vocab_valid),len(vocab_test)))\n",
    "\n",
    "vocab_train=sorted(vocab_train)#must have deterministic ordering, so word2id dictionary is reproducible across invocations\n",
    "\n",
    "word2id={w:i for i,w in enumerate(vocab_train)}\n",
    "id2word={i:w for i,w in enumerate(vocab_train)}\n",
    "\n",
    "ids_train=np.array([word2id[word] for word in seq_train],copy=False,order='C')\n",
    "ids_valid=np.array([word2id[word] for word in seq_valid],copy=False,order='C')\n",
    "ids_test=np.array([word2id[word] for word in seq_test],copy=False,order='C')\n",
    "\n",
    "data_train, steps_train, trainset_np = features_labels(ids_train, batch_size, seq_len, batch_first=False)\n",
    "data_valid, steps_valid, valset_np   = features_labels(ids_valid, batch_size, seq_len, batch_first=False)\n",
    "data_test,  steps_test, testset_np   = features_labels(ids_test,  batch_size, seq_len, batch_first=False)\n",
    "\n",
    "trainset_np = trainset_np / len(vocab_train)\n",
    "valset_np   = valset_np / len(vocab_valid)\n",
    "testset_np  = testset_np / len(vocab_test)\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(data_train).batch(seq_len, drop_remainder=True)\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices(data_valid).batch(seq_len, drop_remainder=True)\n",
    "dataset_test  = tf.data.Dataset.from_tensor_slices(data_test).batch(seq_len,  drop_remainder=True)\n",
    "\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_train}.train.csv\",'w') as csvfile:\n",
    "    np.savetxt(csvfile, np.array([[seq_len, seq_len]]),fmt='%d', delimiter=\",\")\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_train}.train.csv\",'a') as csvfile:\n",
    "    np.savetxt(csvfile, trainset_np, fmt='%.4f', delimiter=\",\")\n",
    "\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_valid}.val.csv\",'w') as csvfile:\n",
    "    np.savetxt(csvfile, np.array([[seq_len, seq_len]]),fmt='%d', delimiter=\",\")\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_valid}.val.csv\",'a') as csvfile:\n",
    "    np.savetxt(csvfile, valset_np, fmt='%.4f', delimiter=\",\")\n",
    "    \n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_test}.test.csv\",'w') as csvfile:\n",
    "    np.savetxt(csvfile, np.array([[seq_len, seq_len]]),fmt='%d', delimiter=\",\")\n",
    "with open(f\"../../Datasets/4_ptb/ptb.ni={seq_len}.no={seq_len}.ts={1}.bs={steps_test}.test.csv\",'a') as csvfile:\n",
    "    np.savetxt(csvfile, testset_np, fmt='%.4f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e558c-3c2d-4bf7-bf4b-485453d9e519",
   "metadata": {},
   "source": [
    "## Skeleton based Action Recognition (NTU RGB+D) Problem\n",
    "\n",
    "Source: \n",
    "* https://github.com/zibeu/Independently-Recurrent-Neural-Network---IndRNN \n",
    "* https://github.com/Sunnydreamrain/IndRNN_pytorch/\n",
    "* https://gist.github.com/tmatha/905ae0c0d304119851d7432e5b359330\n",
    "\n",
    "Hyperparams: https://arxiv.org/abs/1803.04831\n",
    "\n",
    "Data Information: https://rose1.ntu.edu.sg/dataset/actionRecognition/\n",
    "\n",
    "NTU RGB+D is a large-scale dataset for RGB-D human action recognition. It involves 56,880 samples of 60 action classes collected from 40 subjects. The actions can be generally divided into three categories: 40 daily actions (e.g., drinking, eating, reading), nine health-related actions (e.g., sneezing, staggering, falling down), and 11 mutual actions (e.g., punching, kicking, hugging). These actions take place under 17 different scene conditions corresponding to 17 video sequences (i.e., S001–S017). The actions were captured using three cameras with different horizontal imaging viewpoints, namely, −45∘,0∘, and +45∘. Multi-modality information is provided for action characterization, including depth maps, 3D skeleton joint position, RGB frames, and infrared sequences. The performance evaluation is performed by a cross-subject test that split the 40 subjects into training and test groups, and by a cross-view test that employed one camera (+45∘) for testing, and the other two cameras for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ebd0e3-6f0a-4b88-bfb5-e04d07be8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "SKELETON_DIR = '../../Datasets/5_nturgb+d/nturgb+d_skeletons'\n",
    "NPY_DIR = '../../Datasets/5_nturgb+d/nturgp+d_csv/nturgb_npy'\n",
    "TRAIN_DS = '_train.csv'\n",
    "TEST_DS = '_test.csv'\n",
    "\n",
    "skeleton_files_mask = os.path.join(SKELETON_DIR, '*.skeleton')\n",
    "skeleton_files = glob.glob(skeleton_files_mask)\n",
    "\n",
    "\n",
    "max_frame_count = 300\n",
    "max_joints = 50\n",
    "\n",
    "full_ds = []\n",
    "\n",
    "#for idx, file_name in enumerate(skeleton_files[:568]):\n",
    "for idx, file_name in enumerate(skeleton_files):\n",
    "    if idx%100 == 0:\n",
    "        print(idx)\n",
    "    basename = os.path.basename(file_name)\n",
    "    name = os.path.splitext(basename)[0]\n",
    "    label = name.split('A')[1]\n",
    "    with open(file_name) as f:\n",
    "        framecount = int(f.readline())\n",
    "\n",
    "        sequence_frames = []\n",
    "\n",
    "        for frame in range(framecount):\n",
    "            body_count = int(f.readline())\n",
    "            if body_count <= 0 or body_count>2:\n",
    "                print('continue, no body')\n",
    "                break\n",
    "            joints_xyz = []\n",
    "            for body in range(body_count):\n",
    "                skeleton_info = f.readline()\n",
    "                joint_counts = int(f.readline()) #25\n",
    "                for joint in range(joint_counts):\n",
    "                    joint_info = f.readline()\n",
    "                    joint_info_array = joint_info.split()\n",
    "                    x, y, z = joint_info_array[:3]\n",
    "                    joint_info_xyz = np.array([float(x), float(y), float(z)])\n",
    "                    joints_xyz.append(joint_info_xyz)\n",
    "            pad_joints = max_joints - len(joints_xyz)\n",
    "            joints_xyz = np.array(joints_xyz)\n",
    "            joints_xyz = np.pad(joints_xyz, ((0, pad_joints), (0, 0)), mode='constant')\n",
    "            frame_xyz = np.stack(joints_xyz)\n",
    "            sequence_frames.append(frame_xyz)\n",
    "        if len(sequence_frames) > 0:\n",
    "            file_name = os.path.join(NPY_DIR, name+ '.npy')\n",
    "            sample = [name+'.npy', int(label)-1]\n",
    "            full_ds.append(sample)\n",
    "            np.save(file_name, np.array(sequence_frames))\n",
    "\n",
    "#train_ds = full_ds[:380]\n",
    "#test_ds = full_ds[380:]\n",
    "\n",
    "train_ds = full_ds[:40320]\n",
    "test_ds = full_ds[40320:]\n",
    "\n",
    "with open(os.path.join(NPY_DIR, TRAIN_DS), 'w') as train_ds_file:\n",
    "    writer = csv.writer(train_ds_file, lineterminator='\\n')\n",
    "    writer.writerows(train_ds)\n",
    "\n",
    "with open(os.path.join(NPY_DIR, TEST_DS), 'w') as test_ds_file:\n",
    "    writer = csv.writer(test_ds_file, lineterminator='\\n')\n",
    "    writer.writerows(test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "master_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
