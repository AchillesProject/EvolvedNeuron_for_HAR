{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763b0295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf:  2.7.0\n",
      "tb:  2.7.0\n",
      "C:\\Users\\chaut\\OneDrive - Institutt for Energiteknikk\\0_MasterProject\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, LSTM, MaxPooling2D,AveragePooling2D,GlobalMaxPooling2D, GlobalAveragePooling2D, Flatten, Dropout, Reshape, BatchNormalization, ReLU\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "from tensorflow.keras.backend import eval\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, RobustScaler\n",
    "\n",
    "import tensorboard\n",
    "import keras\n",
    "from keras.utils import tf_utils\n",
    "\n",
    "from functools import partial\n",
    "from matplotlib import rc, style\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import pandas as pd #pd.plotting.register_matplotlib_converters\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import sys, os, math, time, datetime\n",
    "\n",
    "print(\"tf: \", tf.__version__)\n",
    "print(\"tb: \", tensorboard.__version__)\n",
    "print(os.getcwd())\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "style.use(\"seaborn\")\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale = 1)\n",
    "\n",
    "# rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(1)\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "\n",
    "snapshot = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Debugging with Tensorboard\n",
    "logdir=\"logs/fit/rnn_v1_1/\" + snapshot\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "# tf.debugging.experimental.enable_dump_debug_info(logdir, tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "\n",
    "path = './Version9.128timesteps'\n",
    "fileslist = [f for f in sorted(os.listdir(path)) if os.path.isfile(os.path.join(path, f))]\n",
    "FILESNUMBER = 1\n",
    "LSTMNUMBER  = 1\n",
    "\n",
    "tinitialLearningRate=0.001172\n",
    "tlearningRateDecay = 0.006226\n",
    "tdecayDurationFactor = 0.98628\n",
    "tglorotScaleFactor = 0.1\n",
    "torthogonalScaleFactor = 0.1\n",
    "tnumTrainingSteps = 40000\n",
    "tbeta1 = 0.943377\n",
    "tbeta2 = 0.97205\n",
    "tepsilon = 8.62125e-5\n",
    "\n",
    "def seperateValues(data):\n",
    "    x_data, y_data = None, None\n",
    "    for i in range(data.shape[0]):\n",
    "        x_data_i = data[i].reshape(-1, noInput+noOutput).astype('float32')\n",
    "        x_data_i, y_data_i = x_data_i[:, 0:noInput], x_data_i[-1, noInput:]\n",
    "        x_data = x_data_i[np.newaxis,:,:] if x_data is None else np.append(x_data, x_data_i[np.newaxis,:,:], axis=0)\n",
    "        y_data = y_data_i.reshape(1, -1) if y_data is None else np.append(y_data, y_data_i.reshape(1, -1), axis=0)\n",
    "    return x_data, y_data\n",
    "\n",
    "def fromBit( b ) :\n",
    "    if b == 0.0 :\n",
    "        return -0.9\n",
    "    return 0.9\n",
    "\n",
    "def isCorrect( target, actual ) :\n",
    "    if target < 0.0 :\n",
    "        y1 = False\n",
    "    else :\n",
    "        y1 = True\n",
    "    if actual < 0.0 :\n",
    "        y2 = False\n",
    "    else :\n",
    "        y2 = True\n",
    "    return y1 == y2 \n",
    "\n",
    "\n",
    "def customMetricfn(true, pred):\n",
    "    count = 0\n",
    "    numCorrect = 0\n",
    "    for i in range( pred.shape[ 0 ] ) :\n",
    "        for j in range( pred.shape[ 1 ] ) :\n",
    "            count += 1\n",
    "            if isCorrect( true[ i, j ], pred[ i, j ] ) :\n",
    "                numCorrect += 1\n",
    "    return (numCorrect/count)\n",
    "\n",
    "def srelu(x):\n",
    "    return tf.keras.backend.clip(x, -1, 1)\n",
    "\n",
    "class customLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initialLearningRate, learningRateDecay, decayDurationFactor, numTrainingSteps, glorotScaleFactor=0.1, orthogonalScaleFactor=0.1, name=None):\n",
    "        self.initialLearningRate = initialLearningRate\n",
    "        self.learningRateDecay = learningRateDecay\n",
    "        self.decayDurationFactor = decayDurationFactor\n",
    "        self.glorotScaleFactor = glorotScaleFactor\n",
    "        self.orthogonalScaleFactor = orthogonalScaleFactor\n",
    "        self.numTrainingSteps = numTrainingSteps\n",
    "        self.name = name\n",
    "        self.T = tf.constant(self.decayDurationFactor * self.numTrainingSteps, dtype=tf.float32, name=\"T\")\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        self.step = tf.cast(step, tf.float32)\n",
    "        self.lr = tf.cond(self.step > self.T, \n",
    "                           lambda: tf.constant(self.learningRateDecay * self.initialLearningRate, dtype=tf.float32),\n",
    "                           lambda: self.initialLearningRate * (1.0 - (1.0 - self.learningRateDecay) * self.step / self.T)\n",
    "                          )\n",
    "        return self.lr\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initialLearningRate\": self.initialLearningRate,\n",
    "            \"learningRateDecay\": self.learningRateDecay,\n",
    "            \"decayDurationFactor\": self.decayDurationFactor,\n",
    "            \"numTrainingSteps\": self.numTrainingSteps,\n",
    "            \"glorotScaleFactor\": self.glorotScaleFactor,\n",
    "            \"orthogonalScaleFactor\": self.orthogonalScaleFactor,\n",
    "            \"T\": self.T,\n",
    "            \"name\": self.name,\n",
    "            \"learningRate\": self.lr\n",
    "        }\n",
    "\n",
    "class DropoutRNNCellMixin:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._create_non_trackable_mask_cache()\n",
    "        super(DropoutRNNCellMixin, self).__init__(*args, **kwargs)\n",
    "\n",
    "    @tf.__internal__.tracking.no_automatic_dependency_tracking\n",
    "    def _create_non_trackable_mask_cache(self):\n",
    "        self._dropout_mask_cache = keras.backend.ContextValueCache(self._create_dropout_mask)\n",
    "        self._recurrent_dropout_mask_cache = keras.backend.ContextValueCache(self._create_recurrent_dropout_mask)\n",
    "\n",
    "    def reset_dropout_mask(self):\n",
    "        self._dropout_mask_cache.clear()\n",
    "\n",
    "    def reset_recurrent_dropout_mask(self):\n",
    "        self._recurrent_dropout_mask_cache.clear()\n",
    "\n",
    "    def _create_dropout_mask(self, inputs, training, count=1):\n",
    "        return _generate_dropout_mask(self._random_generator, tf.ones_like(inputs), self.dropout, training=training, count=count)\n",
    "\n",
    "    def _create_recurrent_dropout_mask(self, inputs, training, count=1):\n",
    "        return _generate_dropout_mask(self._random_generator, tf.ones_like(inputs), self.recurrent_dropout, training=training, count=count)\n",
    "\n",
    "    def get_dropout_mask_for_cell(self, inputs, training, count=1):\n",
    "        if self.dropout == 0:\n",
    "            return None\n",
    "        init_kwargs = dict(inputs=inputs, training=training, count=count)\n",
    "        return self._dropout_mask_cache.setdefault(kwargs=init_kwargs)\n",
    "\n",
    "    def get_recurrent_dropout_mask_for_cell(self, inputs, training, count=1):\n",
    "        if self.recurrent_dropout == 0:\n",
    "            return None\n",
    "        init_kwargs = dict(inputs=inputs, training=training, count=count)\n",
    "        return self._recurrent_dropout_mask_cache.setdefault(kwargs=init_kwargs)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = super(DropoutRNNCellMixin, self).__getstate__()\n",
    "        state.pop('_dropout_mask_cache', None)\n",
    "        state.pop('_recurrent_dropout_mask_cache', None)\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        state['_dropout_mask_cache'] = keras.backend.ContextValueCache(self._create_dropout_mask)\n",
    "        state['_recurrent_dropout_mask_cache'] = keras.backend.ContextValueCache(self._create_recurrent_dropout_mask)\n",
    "        super(DropoutRNNCellMixin, self).__setstate__(state)\n",
    "    \n",
    "class RNN_plus_v1_cell(DropoutRNNCellMixin, keras.engine.base_layer.Layer):\n",
    "    def __init__(self, units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', dropout=0., recurrent_dropout=0., use_bias=True, **kwargs):\n",
    "        if units < 0:\n",
    "            raise ValueError(f'Received an invalid value for argument `units`, '\n",
    "                                f'expected a positive integer, got {units}.')\n",
    "        # By default use cached variable under v2 mode, see b/143699808.\n",
    "        if tf.compat.v1.executing_eagerly_outside_functions():\n",
    "            self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n",
    "        else:\n",
    "            self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n",
    "        super(RNN_plus_v1_cell, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.state_size = self.units\n",
    "        self.output_size = self.units\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = tf.keras.initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        self.use_bias = True\n",
    "    \n",
    "    @tf_utils.shape_type_conversion\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units), name='w_i', initializer=self.kernel_initializer, regularizer=None, constraint=None)\n",
    "        self.recurrent_kernel = self.add_weight(shape=(self.units, self.units), name='w_o', initializer=self.recurrent_initializer, regularizer=None, constraint=None)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight( shape=(self.units,), name='b', initializer=self.bias_initializer, regularizer=None, constraint=None)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, inputs, states, training=None):\n",
    "        prev_output = states[0] if tf.nest.is_nested(states) else states\n",
    "        dp_mask = self.get_dropout_mask_for_cell(inputs, training)\n",
    "        rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(prev_output, training)\n",
    "        if dp_mask is not None:\n",
    "            i = tf.keras.backend.dot(inputs * dp_mask, self.kernel)\n",
    "        else:\n",
    "            i = tf.keras.backend.dot(inputs, self.kernel)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            i = tf.keras.backend.bias_add(i, self.bias)\n",
    "    \n",
    "        if rec_dp_mask is not None:\n",
    "            prev_output = prev_output * rec_dp_mask\n",
    "\n",
    "        z = tf.keras.backend.dot(prev_output, tf.linalg.set_diag(self.recurrent_kernel, np.zeros((self.units,), dtype=int)))\n",
    "        iz = tf.math.add(i, z, name='add_iz')\n",
    "        v = tf.math.subtract(tf.math.square(iz,name='square_iz'), iz, name='sub_v')\n",
    "        output = srelu(v)\n",
    "\n",
    "        new_state = [output] if tf.nest.is_nested(states) else output\n",
    "        return output, new_state\n",
    "\n",
    "def rnn_plus_model():\n",
    "    \"\"\"Builds a recurrent model.\"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(128, 3), name='Input_layer'))\n",
    "    model.add(tf.keras.layers.RNN(cell=RNN_plus_v1_cell(units=8), unroll=False, name='RNNp_layer'))\n",
    "    model.add(tf.keras.layers.Dense(5,'tanh', name='MLP_layer'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=customLRSchedule(tinitialLearningRate, tlearningRateDecay, tdecayDurationFactor, tnumTrainingSteps), beta_1=tbeta1, beta_2=tbeta2, epsilon=tepsilon, amsgrad=False, name=\"tunedAdam\")\n",
    "    model.compile(optimizer=optimizer, loss = 'mse', metrics=[customMetricfn], run_eagerly=False)\n",
    "    return model\n",
    "\n",
    "def rnn_model():\n",
    "    \"\"\"Builds a recurrent model.\"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(128, 3),name='Input_layer'))\n",
    "    model.add(tf.keras.layers.RNN(cell=tf.keras.layers.SimpleRNNCell(units=8), name='SimpleRNN_layer'))\n",
    "    model.add(tf.keras.layers.Dense(5,'tanh', name='MLP_layer'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=customLRSchedule(tinitialLearningRate, tlearningRateDecay, tdecayDurationFactor, tnumTrainingSteps), beta_1=tbeta1, beta_2=tbeta2, epsilon=tepsilon, amsgrad=False, name=\"tunedAdam\")\n",
    "    model.compile(optimizer=optimizer, loss = 'mse', metrics=[customMetricfn], run_eagerly=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965f99b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ seqnetdata.ni=3.no=5.mc=15.numTimeSteps128.version9.0.csv\n",
      "Step 1: Dividing the training and testing set with ratio 1:1 (50%).\n",
      "(5000, 1024) (4999, 1024)\n",
      "Step 2: Separating values and labels.\n",
      "+ Training set:    (5000, 128, 3) (5000, 5) float32\n",
      "+ Validating set:  (4999, 128, 3) (4999, 5) float32\n"
     ]
    }
   ],
   "source": [
    "filename = fileslist[0]\n",
    "filepath = os.path.join(path,filename)\n",
    "with open(filepath, \"r\") as fp:\n",
    "    [noInput, noOutput] = [int(x) for x in fp.readline().split(',')]\n",
    "\n",
    "print('+++', filename)\n",
    "rdf = np.array(pd.read_csv(filepath, skiprows=1))\n",
    "\n",
    "print('Step 1: Dividing the training and testing set with ratio 1:1 (50%).')\n",
    "df_val, df_train = train_test_split(rdf,test_size=0.5)\n",
    "print(df_train.shape, df_val.shape)\n",
    "\n",
    "print('Step 2: Separating values and labels.')\n",
    "x_train, y_train = seperateValues(df_train)\n",
    "x_val, y_val = seperateValues(df_val)\n",
    "for i in range( x_train.shape[ 0 ] ) :\n",
    "    for j in range( x_train.shape[ 1 ] ) :\n",
    "        for k in range( x_train.shape[ 2 ] ) :\n",
    "            x_train[ i, j, k ] = fromBit( x_train[ i, j, k ] )\n",
    "\n",
    "for i in range( y_train.shape[ 0 ] ) :\n",
    "    for j in range( y_train.shape[ 1 ] ) :\n",
    "        y_train[ i, j ] = fromBit( y_train[ i, j ] )\n",
    "\n",
    "for i in range( x_val.shape[ 0 ] ) :\n",
    "    for j in range( x_val.shape[ 1 ] ) :\n",
    "        for k in range( x_val.shape[ 2 ] ) :\n",
    "            x_val[ i, j, k ] = fromBit( x_val[ i, j, k ] )\n",
    "\n",
    "for i in range( y_val.shape[ 0 ] ) :\n",
    "    for j in range( y_val.shape[ 1 ] ) :\n",
    "        y_val[ i, j ] = fromBit( y_val[ i, j ] )\n",
    "\n",
    "print(\"+ Training set:   \", x_train.shape, y_train.shape, x_train.dtype)\n",
    "print(\"+ Validating set: \", x_val.shape, y_val.shape, x_val.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f90c63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "5000/5000 [==============================] - 106s 21ms/step - loss: 0.7857 - customMetricfn: 0.5668 - val_loss: 0.6964 - val_customMetricfn: 0.6113\n",
      "Epoch 2/8\n",
      "5000/5000 [==============================] - 83s 17ms/step - loss: 0.6529 - customMetricfn: 0.6371 - val_loss: 0.6135 - val_customMetricfn: 0.6509\n",
      "4999/4999 [==============================] - 18s 4ms/step - loss: 0.6135 - customMetricfn: 0.6509\n",
      "157/157 [==============================] - 1s 4ms/step\n",
      "0.65093\n"
     ]
    }
   ],
   "source": [
    "model = rnn_plus_model()\n",
    "model_history = model.fit(\n",
    "            x_train, # input\n",
    "            y_train, # output\n",
    "            batch_size=1,\n",
    "            verbose=1, # Suppress chatty output; use Tensorboard instead\n",
    "            epochs=8,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(\"val_customMetricfn\"), tf.keras.callbacks.TerminateOnNaN(), tensorboard_callback]\n",
    "        )\n",
    "\n",
    "val_performance = model.evaluate(x_val, y_val, batch_size=1, verbose=1)\n",
    "y_pred = model.predict(x_val, verbose=1)\n",
    "\n",
    "print(round(customMetricfn(y_val, y_pred), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608596a-42da-4e4c-a12c-d9d886666b19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "5000/5000 [==============================] - 63s 12ms/step - loss: 0.7678 - customMetricfn: 0.5604 - val_loss: 0.7271 - val_customMetricfn: 0.5822\n",
      "Epoch 2/8\n",
      "5000/5000 [==============================] - 71s 14ms/step - loss: 0.7103 - customMetricfn: 0.5965 - val_loss: 0.7005 - val_customMetricfn: 0.6021\n",
      "4999/4999 [==============================] - 18s 4ms/step - loss: 0.7005 - customMetricfn: 0.6021\n",
      "157/157 [==============================] - 1s 4ms/step\n",
      "0.60208\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback1 = tf.keras.callbacks.TensorBoard(log_dir=logdir+ \"_rnn\")\n",
    "model = rnn_model()\n",
    "model_history = model.fit(\n",
    "            x_train, # input\n",
    "            y_train, # output\n",
    "            batch_size=1,\n",
    "            verbose=1, # Suppress chatty output; use Tensorboard instead\n",
    "            epochs=8,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(\"val_customMetricfn\"), tf.keras.callbacks.TerminateOnNaN(), tensorboard_callback1]\n",
    "        )\n",
    "\n",
    "val_performance = model.evaluate(x_val, y_val, batch_size=1, verbose=1)\n",
    "y_pred = model.predict(x_val, verbose=1)\n",
    "\n",
    "print(round(customMetricfn(y_val, y_pred), 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "master_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
