{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d419a4d2-73e3-4bf3-afca-92b91c7c2081",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#006400; color:white; padding:0px 10px; border-radius:5px;\"><h1 style='margin:15px 15px; color:#FFFFFF; font-size:32px'>Recurrent Neuron Network Plus</h1></div>\n",
    "\n",
    "The work is under the **\"Master Thesis\"** by **Chau Tran** with the supervision from **Prof. Roland Olsson**.\n",
    "\n",
    "v1_5:\n",
    "* 04/01/2021: Testing RNN plus in real-world problems (datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e524ac91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf:  2.7.0\n",
      "tb:  2.7.0\n",
      "C:\\Users\\chaut\\OneDrive - Heriot-Watt University\\HIOF_Master\\Master_Thesis\\NewLSTM\\Codes\\tf_implementations\n",
      "{'batchSize': 4.0, 'numTrainingSteps': 40000.0, 'beta1': 0.943377, 'beta2': 0.97205, 'epsilon': 8.62125e-05, 'decayDurationFactor': 0.98628, 'initialLearningRate': 0.002, 'learningRateDecay': 0.006226, 'glorotScaleFactor': 0.1, 'orthogonalScaleFactor': 0.1, 'testSize': 0.5}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorboard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os, math, time, datetime, re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"tf: \", tf.__version__)\n",
    "print(\"tb: \", tensorboard.__version__)\n",
    "print(os.getcwd())\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "ISMOORE_DATASETS = False\n",
    "\n",
    "# Debugging with Tensorboard\n",
    "snapshot = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir=\"../logs/fit/rnn_v1_1/\" + snapshot\n",
    "\n",
    "path = \"../../Datasets/2_addingproblem\" #\"../../Datasets/0_180_small_datasets/Version9.128timesteps\"\n",
    "fileslist = [f for f in sorted(os.listdir(path)) if os.path.isfile(os.path.join(path, f))]\n",
    "\n",
    "with open(\"./params/params_addingproblem.txt\") as f:\n",
    "    hyperparams = dict([re.sub('['+' ,\\n'+']','',x.replace(' .', '')).split('=') for x in f][1:-1])\n",
    "hyperparams = dict([k, float(v)] for k, v in hyperparams.items())\n",
    "hyperparams['testSize'] = 0.500\n",
    "print(hyperparams)\n",
    "\n",
    "def seperateValues(data, noInput, noOutput, isMoore=True):\n",
    "    x_data, y_data = None, None\n",
    "    for i in range(data.shape[0]):\n",
    "        if isMoore:\n",
    "            x_data_i = data[i].reshape(-1, noInput+noOutput)\n",
    "            x_data_i, y_data_i = x_data_i[:, 0:noInput], x_data_i[-1, noInput:]\n",
    "        else:\n",
    "            x_data_i = data[i][:-1].reshape(-1, noInput)\n",
    "            y_data_i = data[i][-1].reshape(-1, noOutput)\n",
    "        x_data = x_data_i[np.newaxis,:,:] if x_data is None else np.append(x_data, x_data_i[np.newaxis,:,:], axis=0)\n",
    "        y_data = y_data_i.reshape(1, -1) if y_data is None else np.append(y_data, y_data_i.reshape(1, -1), axis=0)\n",
    "    return x_data, y_data\n",
    "\n",
    "def fromBit( b ) :\n",
    "    return -0.9 if b == 0.0 else 0.9\n",
    "\n",
    "class CustomMetricError(tf.keras.metrics.MeanMetricWrapper):\n",
    "    def __init__(self, name='custom_metric_error', dtype=None, threshold=0.5):\n",
    "        super(CustomMetricError, self).__init__(\n",
    "            customMetricfn_tensor, name, dtype=dtype, threshold=threshold)\n",
    "\n",
    "def customMetricfn_tensor(true, pred, threshold=0.5):\n",
    "    true = tf.convert_to_tensor(true)\n",
    "    pred = tf.convert_to_tensor(pred)\n",
    "    threshold = tf.cast(threshold, pred.dtype)\n",
    "    pred = tf.cast(pred >= threshold, pred.dtype)\n",
    "    true = tf.cast(true >= threshold, true.dtype)\n",
    "    return keras.backend.mean(tf.equal(true, pred), axis=-1)\n",
    "\n",
    "def customMetricfn(y_true, y_pred):\n",
    "    count, numCorrect = 0, 0\n",
    "    for i in range( y_true.shape[0] ) :\n",
    "        for j in range( y_pred.shape[ 1 ] ) :\n",
    "            count += 1\n",
    "            if isCorrect( y_true[ i, j ], y_pred[ i, j ] ) :\n",
    "                numCorrect += 1\n",
    "    return (numCorrect/count)\n",
    "\n",
    "def isCorrect( target, actual ) :\n",
    "    y1 = False if target < 0.0 else True\n",
    "    y2 = False if actual < 0.0 else True\n",
    "    return y1 == y2 \n",
    "\n",
    "class customLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initialLearningRate, learningRateDecay, decayDurationFactor, numTrainingSteps, glorotScaleFactor=0.1, orthogonalScaleFactor=0.1, name=None):\n",
    "        self.initialLearningRate = initialLearningRate\n",
    "        self.learningRateDecay = learningRateDecay\n",
    "        self.decayDurationFactor = decayDurationFactor\n",
    "        self.glorotScaleFactor = glorotScaleFactor\n",
    "        self.orthogonalScaleFactor = orthogonalScaleFactor\n",
    "        self.numTrainingSteps = numTrainingSteps\n",
    "        self.name = name\n",
    "        self.T = tf.constant(self.decayDurationFactor * self.numTrainingSteps, dtype=tf.float32, name=\"T\")\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        self.step = tf.cast(step, tf.float32)\n",
    "        self.lr = tf.cond(self.step > self.T, \n",
    "                           lambda: tf.constant(self.learningRateDecay * self.initialLearningRate, dtype=tf.float32),\n",
    "                           lambda: self.initialLearningRate * (1.0 - (1.0 - self.learningRateDecay) * self.step / self.T)\n",
    "                          )\n",
    "        return self.lr\n",
    "    \n",
    "class RNN_plus_v1_cell(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', dropout=0., recurrent_dropout=0., use_bias=True, **kwargs):\n",
    "        if units < 0:\n",
    "            raise ValueError(f'Received an invalid value for argument `units`, '\n",
    "                                f'expected a positive integer, got {units}.')\n",
    "        # By default use cached variable under v2 mode, see b/143699808.\n",
    "        if tf.compat.v1.executing_eagerly_outside_functions():\n",
    "            self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n",
    "        else:\n",
    "            self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n",
    "        super(RNN_plus_v1_cell, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.state_size = self.units\n",
    "        self.output_size = self.units\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = tf.keras.initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        self.use_bias = True\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units), name='w_i', initializer=self.kernel_initializer, regularizer=None, constraint=None)\n",
    "        self.recurrent_kernel = self.add_weight(shape=(self.units, self.units), name='w_o', initializer=self.recurrent_initializer, regularizer=None, constraint=None)\n",
    "        self.bias = self.add_weight( shape=(self.units,), name='b', initializer=self.bias_initializer, regularizer=None, constraint=None) if self.use_bias else None\n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, inputs, states, training=None):\n",
    "        prev_output = states[0] if tf.nest.is_nested(states) else states\n",
    "        i = tf.keras.backend.dot(inputs, self.kernel)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            i = tf.keras.backend.bias_add(i, self.bias)\n",
    "\n",
    "        z = tf.keras.backend.dot(prev_output, tf.linalg.set_diag(self.recurrent_kernel, np.zeros((self.units,), dtype=int)))\n",
    "        iz = tf.math.add(i, z, name='add_iz')\n",
    "        v = tf.math.subtract(tf.math.square(iz,name='square_iz'), iz, name='sub_v')\n",
    "        output = tf.keras.backend.clip(v, -1, 1)\n",
    "\n",
    "        new_state = [output] if tf.nest.is_nested(states) else output\n",
    "        return output, new_state\n",
    "\n",
    "class RNN_plus_models():\n",
    "    def __init__(self, timestep, noInput, noOutput, batchSize, isLRS=False, isCMF=False):\n",
    "        self.timestep = timestep\n",
    "        self.noInput = noInput\n",
    "        self.noOutput = noOutput\n",
    "        self.batchSize = batchSize\n",
    "        self.isLRS = isLRS\n",
    "        self.isCMF = isCMF\n",
    "    \n",
    "    def rnn_plus_choose_models(self):\n",
    "        if (self.isLRS and self.isCMF):\n",
    "            return self.rnn_plus_wLRS_wCMF_model()\n",
    "        elif (self.isLRS and not self.isCMF):\n",
    "            return self.rnn_plus_wLRS_wtCMF_model()\n",
    "        elif (not self.isLRS and self.isCMF):\n",
    "            return self.rnn_plus_wtLRS_wCMF_model()\n",
    "        else:\n",
    "            return self.rnn_plus_wtLRS_wtCMF_model()\n",
    "        \n",
    "    def rnn_plus_wLRS_wCMF_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.RNN(cell=RNN_plus_v1_cell(units=self.noInput+self.noOutput), input_shape=[self.timestep, self.noInput], unroll=False, name='RNNp_layer'))\n",
    "        model.add(tf.keras.layers.Dense(self.noOutput, activation='tanh', name='MLP_layer'))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=customLRSchedule(hyperparams['initialLearningRate'], hyperparams['learningRateDecay'], hyperparams['decayDurationFactor'], hyperparams['numTrainingSteps']), \\\n",
    "                                            beta_1=hyperparams['beta1'], beta_2=hyperparams['beta2'], epsilon=hyperparams['epsilon'], amsgrad=False, name=\"tunedAdam\")\n",
    "        model.compile(optimizer=optimizer, loss = 'mse', metrics=[CustomMetricError(threshold=0.0)], run_eagerly=False)\n",
    "        return model\n",
    "\n",
    "    def rnn_plus_wLRS_wtCMF_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.RNN(cell=RNN_plus_v1_cell(units=self.noInput+self.noOutput), input_shape=[self.timestep, self.noInput], unroll=False, name='RNNp_layer'))\n",
    "        model.add(tf.keras.layers.Dense(self.noOutput, activation='tanh', name='MLP_layer'))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=customLRSchedule(hyperparams['initialLearningRate'], hyperparams['learningRateDecay'], hyperparams['decayDurationFactor'], hyperparams['numTrainingSteps']), \\\n",
    "                                            beta_1=hyperparams['beta1'], beta_2=hyperparams['beta2'], epsilon=hyperparams['epsilon'], amsgrad=False, name=\"tunedAdam\")\n",
    "        model.compile(optimizer=optimizer, loss = 'mse')\n",
    "        return model\n",
    "\n",
    "    def rnn_plus_wtLRS_wCMF_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.RNN(cell=RNN_plus_v1_cell(units=self.noInput+self.noOutput), input_shape=[self.timestep, self.noInput], unroll=False, name='RNNp_layer'))\n",
    "        model.add(tf.keras.layers.Dense(self.noOutput, activation='tanh', name='MLP_layer'))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=0, name=\"Adam_wtlrs\")\n",
    "        model.compile(optimizer=optimizer, loss = 'mse', metrics=[CustomMetricError(threshold=0.0)], run_eagerly=False)\n",
    "        return model\n",
    "\n",
    "    def rnn_plus_wtLRS_wtCMF_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.RNN(cell=RNN_plus_v1_cell(units=self.noInput+self.noOutput), input_shape=[self.timestep, self.noInput], unroll=False, name='RNNp_layer'))\n",
    "        model.add(tf.keras.layers.Dense(self.noOutput, activation='tanh', name='MLP_layer'))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=0, name=\"Adam_wtlrs\")\n",
    "        model.compile(optimizer=optimizer, loss = 'mse')\n",
    "        return model\n",
    "        \n",
    "class RNN_models():\n",
    "    def __init__(self, timestep, noInput, noOutput, batchSize, isLRS=False, isCMF=False):\n",
    "        self.timestep = timestep\n",
    "        self.noInput = noInput\n",
    "        self.noOutput = noOutput\n",
    "        self.batchSize = batchSize\n",
    "        self.isLRS = isLRS\n",
    "        self.isCMF = isCMF\n",
    "        \n",
    "    def rnn_choose_models(self):\n",
    "        if (self.isLRS and self.isCMF):\n",
    "            return self.rnn_wLRS_wCMF_model()\n",
    "        elif (self.isLRS and not self.isCMF):\n",
    "            return self.rnn_wLRS_wtCMF_model()\n",
    "        elif (not self.isLRS and self.isCMF):\n",
    "            return self.rnn_wtLRS_wCMF_model()\n",
    "        else:\n",
    "            return self.rnn_wtLRS_wtCMF_model()\n",
    "        \n",
    "    def rnn_wLRS_wCMF_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.RNN(cell=tf.keras.layers.SimpleRNNCell(units=self.noInput+self.noOutput), input_shape=[self.timestep, self.noInput], name='SimpleRNN_layer', stateful=False))\n",
    "        model.add(tf.keras.layers.Dense(self.noOutput, activation='tanh', name='MLP_layer'))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=customLRSchedule(hyperparams['initialLearningRate'], hyperparams['learningRateDecay'], hyperparams['decayDurationFactor'], hyperparams['numTrainingSteps']), \\\n",
    "                                            beta_1=hyperparams['beta1'], beta_2=hyperparams['beta2'], epsilon=hyperparams['epsilon'], amsgrad=False, name=\"tunedAdam_rnn\")\n",
    "        model.compile(optimizer=optimizer, loss = 'mse', metrics=[CustomMetricError(threshold=0.0)], run_eagerly=False)\n",
    "        return model\n",
    "\n",
    "    def rnn_wLRS_wtCMF_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.RNN(cell=tf.keras.layers.SimpleRNNCell(units=self.noInput+self.noOutput), input_shape=[self.timestep, self.noInput], name='SimpleRNN_layer', stateful=False))\n",
    "        model.add(tf.keras.layers.Dense(self.noOutput, activation='tanh', name='MLP_layer'))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=customLRSchedule(hyperparams['initialLearningRate'], hyperparams['learningRateDecay'], hyperparams['decayDurationFactor'], hyperparams['numTrainingSteps']), \\\n",
    "                                            beta_1=hyperparams['beta1'], beta_2=hyperparams['beta2'], epsilon=hyperparams['epsilon'], amsgrad=False, name=\"tunedAdam_rnn\")\n",
    "        model.compile(optimizer=optimizer, loss = 'mse')\n",
    "        return model\n",
    "    \n",
    "    def rnn_wtLRS_wCMF_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.RNN(cell=tf.keras.layers.SimpleRNNCell(units=self.noInput+self.noOutput), input_shape=[self.timestep, self.noInput], name='SimpleRNN_layer', stateful=False))\n",
    "        model.add(tf.keras.layers.Dense(self.noOutput, activation='tanh', name='MLP_layer'))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=0, name=\"Adam_wtlrs\")\n",
    "        model.compile(optimizer=optimizer, loss = 'mse', metrics=[CustomMetricError(threshold=0.0)], run_eagerly=False)\n",
    "        return model\n",
    "    \n",
    "    def rnn_wtLRS_wtCMF_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.RNN(cell=tf.keras.layers.SimpleRNNCell(units=self.noInput+self.noOutput), input_shape=[self.timestep, self.noInput], name='SimpleRNN_layer', stateful=False))\n",
    "        model.add(tf.keras.layers.Dense(self.noOutput, activation='tanh', name='MLP_layer'))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=0, name=\"Adam_wtlrs\")\n",
    "        model.compile(optimizer=optimizer, loss = 'mse')\n",
    "        return model\n",
    "\n",
    "class LSTM_models():\n",
    "    def __init__(self, timestep, noInput, noOutput, batchSize, isLRS=False, isCMF=False):\n",
    "        self.timestep = timestep\n",
    "        self.noInput = noInput\n",
    "        self.noOutput = noOutput\n",
    "        self.batchSize = batchSize\n",
    "        self.isLRS = isLRS\n",
    "        self.isCMF = isCMF\n",
    "        \n",
    "    def lstm_choose_models():\n",
    "        if (self.isLRS and self.isCMF):\n",
    "            return self.lstm_wLRS_wCMF_model()\n",
    "        elif (self.isLRS and not self.isCMF):\n",
    "            return self.lstm_wLRS_wtCMF_model()\n",
    "        elif (not self.isLRS and self.isCMF):\n",
    "            return self.lstm_wtLRS_wCMF_model()\n",
    "        else:\n",
    "            return self.lstm_wtLRS_wtCMF_model()\n",
    "        \n",
    "    def lstm_wLRS_wCMF_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.LSTM(units=self.noInput+self.noOutput, input_shape=[self.timestep, self.noInput],\n",
    "                       activation='tanh', recurrent_activation='sigmoid', unroll =False, use_bias=True,\n",
    "                       recurrent_dropout=0.0, return_sequences=False))\n",
    "        model.add(tf.keras.layers.Dense(self.noOutput, activation='tanh', name='MLP_layer'))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=customLRSchedule(hyperparams['initialLearningRate'], hyperparams['learningRateDecay'], hyperparams['decayDurationFactor'], hyperparams['numTrainingSteps']), \\\n",
    "                                        beta_1=hyperparams['beta1'], beta_2=hyperparams['beta2'], epsilon=hyperparams['epsilon'], amsgrad=False, name=\"tunedAdam_lstm\")\n",
    "        model.compile(optimizer=optimizer, loss = 'mse', metrics=[CustomMetricError(threshold=0.0)], run_eagerly=False)\n",
    "        return model\n",
    "\n",
    "    def lstm_wLRS_wtCMF_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.LSTM(units=self.noInput+self.noOutput, input_shape=[self.timestep, self.noInput],\n",
    "                       activation='tanh', recurrent_activation='sigmoid', unroll =False, use_bias=True,\n",
    "                       recurrent_dropout=0.0, return_sequences=False))\n",
    "        model.add(tf.keras.layers.Dense(self.noOutput, activation='tanh', name='MLP_layer'))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=customLRSchedule(hyperparams['initialLearningRate'], hyperparams['learningRateDecay'], hyperparams['decayDurationFactor'], hyperparams['numTrainingSteps']), \\\n",
    "                                        beta_1=hyperparams['beta1'], beta_2=hyperparams['beta2'], epsilon=hyperparams['epsilon'], amsgrad=False, name=\"tunedAdam_lstm\")\n",
    "        model.compile(optimizer=optimizer, loss = 'mse')\n",
    "        return model\n",
    "    \n",
    "    def lstm_wtLRS_wCMF_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.LSTM(units=self.noInput+self.noOutput, input_shape=[self.timestep, self.noInput],\n",
    "                       activation='tanh', recurrent_activation='sigmoid', unroll =False, use_bias=True, \n",
    "                       recurrent_dropout=0.0, return_sequences=False))\n",
    "        model.add(tf.keras.layers.Dense(self.noOutput, activation='tanh', name='MLP_layer'))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=0, name=\"Adam_wtlrs\")\n",
    "        model.compile(optimizer=optimizer, loss = 'mse', metrics=[CustomMetricError(threshold=0.0)], run_eagerly=False)\n",
    "        return model\n",
    "    \n",
    "    def lstm_wtLRS_wtCMF_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.LSTM(units=self.noInput+self.noOutput, input_shape=[self.timestep, self.noInput],\n",
    "                       activation='tanh', recurrent_activation='sigmoid', unroll =False, use_bias=True,\n",
    "                       recurrent_dropout=0.0, return_sequences=False))\n",
    "        model.add(tf.keras.layers.Dense(self.noOutput, activation='tanh', name='MLP_layer'))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=0, name=\"Adam_wtlrs\")\n",
    "        model.compile(optimizer=optimizer, loss = 'mse')\n",
    "        return model\n",
    "\n",
    "class models(RNN_models, RNN_plus_models, LSTM_models):\n",
    "    def __init__(self,  timestep, noInput, noOutput, batchSize, modeltype='RNN_plus', isLRS=False, isCMF=False):\n",
    "        super(models).__init__()\n",
    "        self.modeltype = modeltype\n",
    "        self.timestep = timestep\n",
    "        self.noInput = noInput\n",
    "        self.noOutput = noOutput\n",
    "        self.batchSize = batchSize\n",
    "        self.isLRS = isLRS\n",
    "        self.isCMF = isCMF\n",
    "    \n",
    "    def chooseModels(self):\n",
    "        sLRS = 'wLRS' if self.isLRS else 'wtLRS'\n",
    "        sCMF = 'wCMF' if self.isCMF else 'wtCMF'\n",
    "        prefix = f'{self.modeltype.lower()}_{sLRS}_{sCMF}_tf'\n",
    "        display(prefix)\n",
    "        if self.modeltype == 'LSTM':\n",
    "            return self.lstm_choose_models()\n",
    "        elif self.modeltype == 'RNN_plus':\n",
    "            return self.rnn_plus_choose_models()\n",
    "        else:\n",
    "            return self.rnn_choose_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea12ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ addingProblem.bs=100.ts=100.csv\n",
      "+++ Batch Size:  50\n",
      "Step 1: Dividing the training and testing set with ratio 1:1 (50.0%).\n",
      "+ Training set:    (100, 201) (100, 201)\n",
      "+ Training set:    (100, 100, 2) (100, 1) float64\n",
      "+ Validating set:  (100, 100, 2) (100, 1) float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'rnn_plus_wLRS_wtCMF_tf'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "2/2 [==============================] - 1s 193ms/step - loss: 0.3476 - val_loss: 0.3904\n",
      "Epoch 2/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3304 - val_loss: 0.3752\n",
      "Epoch 3/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3142 - val_loss: 0.3609\n",
      "Epoch 4/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.2990 - val_loss: 0.3473\n",
      "Epoch 5/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.2849 - val_loss: 0.3345\n",
      "Epoch 6/400\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.2714 - val_loss: 0.3223\n",
      "Epoch 7/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2587 - val_loss: 0.3107\n",
      "Epoch 8/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.2466 - val_loss: 0.2996\n",
      "Epoch 9/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.2351 - val_loss: 0.2890\n",
      "Epoch 10/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.2244 - val_loss: 0.2789\n",
      "Epoch 11/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.2141 - val_loss: 0.2693\n",
      "Epoch 12/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2044 - val_loss: 0.2600\n",
      "Epoch 13/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1952 - val_loss: 0.2512\n",
      "Epoch 14/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1863 - val_loss: 0.2427\n",
      "Epoch 15/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1779 - val_loss: 0.2346\n",
      "Epoch 16/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.1699 - val_loss: 0.2268\n",
      "Epoch 17/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1623 - val_loss: 0.2193\n",
      "Epoch 18/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.1550 - val_loss: 0.2122\n",
      "Epoch 19/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1481 - val_loss: 0.2053\n",
      "Epoch 20/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1415 - val_loss: 0.1987\n",
      "Epoch 21/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1352 - val_loss: 0.1924\n",
      "Epoch 22/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1292 - val_loss: 0.1863\n",
      "Epoch 23/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1235 - val_loss: 0.1805\n",
      "Epoch 24/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.1180 - val_loss: 0.1750\n",
      "Epoch 25/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1128 - val_loss: 0.1697\n",
      "Epoch 26/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1079 - val_loss: 0.1646\n",
      "Epoch 27/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.1033 - val_loss: 0.1598\n",
      "Epoch 28/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0988 - val_loss: 0.1552\n",
      "Epoch 29/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0946 - val_loss: 0.1508\n",
      "Epoch 30/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0906 - val_loss: 0.1449\n",
      "Epoch 31/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0869 - val_loss: 0.1386\n",
      "Epoch 32/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0833 - val_loss: 0.1324\n",
      "Epoch 33/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0798 - val_loss: 0.1264\n",
      "Epoch 34/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0765 - val_loss: 0.1206\n",
      "Epoch 35/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0734 - val_loss: 0.1148\n",
      "Epoch 36/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0704 - val_loss: 0.1093\n",
      "Epoch 37/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0674 - val_loss: 0.1039\n",
      "Epoch 38/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0646 - val_loss: 0.0986\n",
      "Epoch 39/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0619 - val_loss: 0.0934\n",
      "Epoch 40/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0593 - val_loss: 0.0884\n",
      "Epoch 41/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0567 - val_loss: 0.0836\n",
      "Epoch 42/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0543 - val_loss: 0.0789\n",
      "Epoch 43/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0519 - val_loss: 0.0744\n",
      "Epoch 44/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0497 - val_loss: 0.0705\n",
      "Epoch 45/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0475 - val_loss: 0.0670\n",
      "Epoch 46/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0454 - val_loss: 0.0635\n",
      "Epoch 47/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0433 - val_loss: 0.0602\n",
      "Epoch 48/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0414 - val_loss: 0.0570\n",
      "Epoch 49/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0395 - val_loss: 0.0539\n",
      "Epoch 50/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0376 - val_loss: 0.0510\n",
      "Epoch 51/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0358 - val_loss: 0.0481\n",
      "Epoch 52/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0341 - val_loss: 0.0453\n",
      "Epoch 53/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0324 - val_loss: 0.0427\n",
      "Epoch 54/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0308 - val_loss: 0.0401\n",
      "Epoch 55/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0292 - val_loss: 0.0377\n",
      "Epoch 56/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0277 - val_loss: 0.0353\n",
      "Epoch 57/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0262 - val_loss: 0.0331\n",
      "Epoch 58/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0248 - val_loss: 0.0310\n",
      "Epoch 59/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0234 - val_loss: 0.0290\n",
      "Epoch 60/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0221 - val_loss: 0.0271\n",
      "Epoch 61/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0208 - val_loss: 0.0252\n",
      "Epoch 62/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0196 - val_loss: 0.0235\n",
      "Epoch 63/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0184 - val_loss: 0.0219\n",
      "Epoch 64/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0173 - val_loss: 0.0203\n",
      "Epoch 65/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0162 - val_loss: 0.0189\n",
      "Epoch 66/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0152 - val_loss: 0.0175\n",
      "Epoch 67/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0142 - val_loss: 0.0162\n",
      "Epoch 68/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0133 - val_loss: 0.0150\n",
      "Epoch 69/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0124 - val_loss: 0.0139\n",
      "Epoch 70/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0115 - val_loss: 0.0128\n",
      "Epoch 71/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 72/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0100 - val_loss: 0.0109\n",
      "Epoch 73/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0092 - val_loss: 0.0100\n",
      "Epoch 74/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0085 - val_loss: 0.0092\n",
      "Epoch 75/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0079 - val_loss: 0.0084\n",
      "Epoch 76/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0073 - val_loss: 0.0077\n",
      "Epoch 77/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 78/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 79/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0056 - val_loss: 0.0058\n",
      "Epoch 80/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0051 - val_loss: 0.0053\n",
      "Epoch 81/400\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0047 - val_loss: 0.0048\n",
      "Epoch 82/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 83/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 84/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 85/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 86/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 87/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 88/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 89/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 90/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 91/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 92/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 93/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 94/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0011 - val_loss: 9.9173e-04\n",
      "Epoch 95/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 9.2716e-04 - val_loss: 8.5301e-04\n",
      "Epoch 96/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 8.0005e-04 - val_loss: 7.2921e-04\n",
      "Epoch 97/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 6.8602e-04 - val_loss: 6.1918e-04\n",
      "Epoch 98/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 5.8479e-04 - val_loss: 5.2187e-04\n",
      "Epoch 99/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.9442e-04 - val_loss: 4.3626e-04\n",
      "Epoch 100/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 4.1437e-04 - val_loss: 3.6141e-04\n",
      "Epoch 101/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 3.4414e-04 - val_loss: 2.9639e-04\n",
      "Epoch 102/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.8221e-04 - val_loss: 2.4022e-04\n",
      "Epoch 103/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.2856e-04 - val_loss: 1.9205e-04\n",
      "Epoch 104/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.8157e-04 - val_loss: 1.5118e-04\n",
      "Epoch 105/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 1.4251e-04 - val_loss: 1.1683e-04\n",
      "Epoch 106/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1.0974e-04 - val_loss: 8.8342e-05\n",
      "Epoch 107/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 8.2429e-05 - val_loss: 6.5071e-05\n",
      "Epoch 108/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 6.0249e-05 - val_loss: 4.7271e-05\n",
      "Epoch 109/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 4.3860e-05 - val_loss: 3.4534e-05\n",
      "Epoch 110/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.1848e-05 - val_loss: 2.4550e-05\n",
      "Epoch 111/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.2525e-05 - val_loss: 1.6859e-05\n",
      "Epoch 112/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.5281e-05 - val_loss: 1.1071e-05\n",
      "Epoch 113/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 9.9069e-06 - val_loss: 6.8454e-06\n",
      "Epoch 114/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 6.0188e-06 - val_loss: 3.8915e-06\n",
      "Epoch 115/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 3.3380e-06 - val_loss: 1.9627e-06\n",
      "Epoch 116/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 1.6512e-06 - val_loss: 8.4896e-07\n",
      "Epoch 117/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6.8509e-07 - val_loss: 3.7311e-07\n",
      "Epoch 118/400\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 3.5825e-07 - val_loss: 3.8528e-07\n",
      "Epoch 119/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 4.6684e-07 - val_loss: 7.5999e-07\n",
      "Epoch 120/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 9.1076e-07 - val_loss: 1.3924e-06\n",
      "Epoch 121/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.5800e-06 - val_loss: 2.1953e-06\n",
      "Epoch 122/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.4207e-06 - val_loss: 3.0976e-06\n",
      "Epoch 123/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 3.3290e-06 - val_loss: 4.0405e-06\n",
      "Epoch 124/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 4.2736e-06 - val_loss: 4.9773e-06\n",
      "Epoch 125/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 5.2022e-06 - val_loss: 5.8709e-06\n",
      "Epoch 126/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6.0835e-06 - val_loss: 6.6930e-06\n",
      "Epoch 127/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6.8830e-06 - val_loss: 7.4218e-06\n",
      "Epoch 128/400\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 7.5850e-06 - val_loss: 8.0419e-06\n",
      "Epoch 129/400\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 8.1741e-06 - val_loss: 8.5431e-06\n",
      "Epoch 130/400\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 8.6458e-06 - val_loss: 8.9208e-06\n",
      "Epoch 131/400\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 8.9919e-06 - val_loss: 9.1731e-06\n",
      "Epoch 132/400\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 9.2129e-06 - val_loss: 9.3017e-06\n",
      "Epoch 133/400\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 9.3112e-06 - val_loss: 9.3114e-06\n",
      "Epoch 134/400\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 9.2922e-06 - val_loss: 9.2089e-06\n",
      "Epoch 135/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 9.1641e-06 - val_loss: 9.0028e-06\n",
      "Epoch 136/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 8.9341e-06 - val_loss: 8.7036e-06\n",
      "Epoch 137/400\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 8.6140e-06 - val_loss: 8.3229e-06\n",
      "Epoch 138/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 8.2134e-06 - val_loss: 7.8734e-06\n",
      "Epoch 139/400\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 7.7511e-06 - val_loss: 7.3670e-06\n",
      "Epoch 140/400\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 7.2332e-06 - val_loss: 6.8174e-06\n",
      "Epoch 141/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 6.6726e-06 - val_loss: 6.2379e-06\n",
      "Epoch 142/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 6.0878e-06 - val_loss: 5.6410e-06\n",
      "Epoch 143/400\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 5.4887e-06 - val_loss: 5.0388e-06\n",
      "Epoch 144/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 4.8909e-06 - val_loss: 4.4426e-06\n",
      "Epoch 145/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 4.2946e-06 - val_loss: 3.8639e-06\n",
      "Epoch 146/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 3.7260e-06 - val_loss: 3.3114e-06\n",
      "Epoch 147/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3.1818e-06 - val_loss: 2.7939e-06\n",
      "Epoch 148/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.6699e-06 - val_loss: 2.3185e-06\n",
      "Epoch 149/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.2107e-06 - val_loss: 1.8899e-06\n",
      "Epoch 150/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.7947e-06 - val_loss: 1.5123e-06\n",
      "Epoch 151/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.4304e-06 - val_loss: 1.1881e-06\n",
      "Epoch 152/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.1196e-06 - val_loss: 9.1773e-07\n",
      "Epoch 153/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 8.5770e-07 - val_loss: 7.0069e-07\n",
      "Epoch 154/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 6.5847e-07 - val_loss: 5.3410e-07\n",
      "Epoch 155/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 4.9903e-07 - val_loss: 4.1488e-07\n",
      "Epoch 156/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 3.9557e-07 - val_loss: 3.3801e-07\n",
      "Epoch 157/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 3.2364e-07 - val_loss: 2.9820e-07\n",
      "Epoch 158/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.9203e-07 - val_loss: 2.8914e-07\n",
      "Epoch 159/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.9369e-07 - val_loss: 3.0435e-07\n",
      "Epoch 160/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3.1358e-07 - val_loss: 3.3725e-07\n",
      "Epoch 161/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3.4949e-07 - val_loss: 3.8137e-07\n",
      "Epoch 162/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 3.9200e-07 - val_loss: 4.3049e-07\n",
      "Epoch 163/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 4.4158e-07 - val_loss: 4.7939e-07\n",
      "Epoch 164/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.9202e-07 - val_loss: 5.2388e-07\n",
      "Epoch 165/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 5.3262e-07 - val_loss: 5.5986e-07\n",
      "Epoch 166/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 5.6629e-07 - val_loss: 5.8521e-07\n",
      "Epoch 167/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5.8891e-07 - val_loss: 5.9855e-07\n",
      "Epoch 168/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 5.9987e-07 - val_loss: 5.9997e-07\n",
      "Epoch 169/400\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 5.9808e-07 - val_loss: 5.8967e-07\n",
      "Epoch 170/400\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 5.8490e-07 - val_loss: 5.6907e-07\n",
      "Epoch 171/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 5.6259e-07 - val_loss: 5.3982e-07\n",
      "Epoch 172/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 5.3190e-07 - val_loss: 5.0481e-07\n",
      "Epoch 173/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 4.9482e-07 - val_loss: 4.6700e-07\n",
      "Epoch 174/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 4.5658e-07 - val_loss: 4.2856e-07\n",
      "Epoch 175/400\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 4.2002e-07 - val_loss: 3.9161e-07\n",
      "Epoch 176/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3.8219e-07 - val_loss: 3.5872e-07\n",
      "Epoch 177/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 3.5236e-07 - val_loss: 3.3089e-07\n",
      "Epoch 178/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 3.2606e-07 - val_loss: 3.0932e-07\n",
      "Epoch 179/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 3.0612e-07 - val_loss: 2.9436e-07\n",
      "Epoch 180/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.9275e-07 - val_loss: 2.8577e-07\n",
      "Epoch 181/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.8558e-07 - val_loss: 2.8285e-07\n",
      "Epoch 182/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.8226e-07 - val_loss: 2.8448e-07\n",
      "Epoch 183/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.8625e-07 - val_loss: 2.8941e-07\n",
      "Epoch 184/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.9164e-07 - val_loss: 2.9622e-07\n",
      "Epoch 185/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.9855e-07 - val_loss: 3.0354e-07\n",
      "Epoch 186/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 3.0565e-07 - val_loss: 3.1017e-07\n",
      "Epoch 187/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 3.1179e-07 - val_loss: 3.1515e-07\n",
      "Epoch 188/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3.1612e-07 - val_loss: 3.1788e-07\n",
      "Epoch 189/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 3.1811e-07 - val_loss: 3.1810e-07\n",
      "Epoch 190/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 3.1776e-07 - val_loss: 3.1568e-07\n",
      "Epoch 191/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 3.1491e-07 - val_loss: 3.1126e-07\n",
      "Epoch 192/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 3.1020e-07 - val_loss: 3.0545e-07\n",
      "Epoch 193/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 3.0352e-07 - val_loss: 2.9915e-07\n",
      "Epoch 194/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.9710e-07 - val_loss: 2.9283e-07\n",
      "Epoch 195/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.9184e-07 - val_loss: 2.8701e-07\n",
      "Epoch 196/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.8527e-07 - val_loss: 2.8249e-07\n",
      "Epoch 197/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.8218e-07 - val_loss: 2.7931e-07\n",
      "Epoch 198/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.7828e-07 - val_loss: 2.7770e-07\n",
      "Epoch 199/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.7703e-07 - val_loss: 2.7739e-07\n",
      "Epoch 200/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.7803e-07 - val_loss: 2.7809e-07\n",
      "Epoch 201/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.7795e-07 - val_loss: 2.7934e-07\n",
      "Epoch 202/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.8012e-07 - val_loss: 2.8078e-07\n",
      "Epoch 203/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.8143e-07 - val_loss: 2.8193e-07\n",
      "Epoch 204/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.8238e-07 - val_loss: 2.8250e-07\n",
      "Epoch 205/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.8268e-07 - val_loss: 2.8230e-07\n",
      "Epoch 206/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.8202e-07 - val_loss: 2.8123e-07\n",
      "Epoch 207/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.8091e-07 - val_loss: 2.7969e-07\n",
      "Epoch 208/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.7929e-07 - val_loss: 2.7770e-07\n",
      "Epoch 209/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.7713e-07 - val_loss: 2.7576e-07\n",
      "Epoch 210/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.7518e-07 - val_loss: 2.7399e-07\n",
      "Epoch 211/400\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 2.7379e-07 - val_loss: 2.7257e-07\n",
      "Epoch 212/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.7218e-07 - val_loss: 2.7166e-07\n",
      "Epoch 213/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.7170e-07 - val_loss: 2.7122e-07\n",
      "Epoch 214/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.7132e-07 - val_loss: 2.7113e-07\n",
      "Epoch 215/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.7108e-07 - val_loss: 2.7111e-07\n",
      "Epoch 216/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.7110e-07 - val_loss: 2.7103e-07\n",
      "Epoch 217/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.7104e-07 - val_loss: 2.7076e-07\n",
      "Epoch 218/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.7076e-07 - val_loss: 2.7023e-07\n",
      "Epoch 219/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.7022e-07 - val_loss: 2.6943e-07\n",
      "Epoch 220/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.6942e-07 - val_loss: 2.6844e-07\n",
      "Epoch 221/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.6845e-07 - val_loss: 2.6738e-07\n",
      "Epoch 222/400\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 2.6689e-07 - val_loss: 2.6642e-07\n",
      "Epoch 223/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.6652e-07 - val_loss: 2.6548e-07\n",
      "Epoch 224/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.6501e-07 - val_loss: 2.6475e-07\n",
      "Epoch 225/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.6493e-07 - val_loss: 2.6412e-07\n",
      "Epoch 226/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.6373e-07 - val_loss: 2.6360e-07\n",
      "Epoch 227/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.6379e-07 - val_loss: 2.6310e-07\n",
      "Epoch 228/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.6276e-07 - val_loss: 2.6253e-07\n",
      "Epoch 229/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.6263e-07 - val_loss: 2.6188e-07\n",
      "Epoch 230/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 2.6191e-07 - val_loss: 2.6112e-07\n",
      "Epoch 231/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.6108e-07 - val_loss: 2.6024e-07\n",
      "Epoch 232/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.6016e-07 - val_loss: 2.5931e-07\n",
      "Epoch 233/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.5903e-07 - val_loss: 2.5837e-07\n",
      "Epoch 234/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.5812e-07 - val_loss: 2.5752e-07\n",
      "Epoch 235/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.5744e-07 - val_loss: 2.5669e-07\n",
      "Epoch 236/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.5646e-07 - val_loss: 2.5597e-07\n",
      "Epoch 237/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.5571e-07 - val_loss: 2.5525e-07\n",
      "Epoch 238/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.5495e-07 - val_loss: 2.5448e-07\n",
      "Epoch 239/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.5450e-07 - val_loss: 2.5352e-07\n",
      "Epoch 240/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.5311e-07 - val_loss: 2.5261e-07\n",
      "Epoch 241/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.5214e-07 - val_loss: 2.5164e-07\n",
      "Epoch 242/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.5114e-07 - val_loss: 2.5065e-07\n",
      "Epoch 243/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.5013e-07 - val_loss: 2.4967e-07\n",
      "Epoch 244/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.4981e-07 - val_loss: 2.4868e-07\n",
      "Epoch 245/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.4817e-07 - val_loss: 2.4775e-07\n",
      "Epoch 246/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.4725e-07 - val_loss: 2.4679e-07\n",
      "Epoch 247/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.4631e-07 - val_loss: 2.4580e-07\n",
      "Epoch 248/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.4532e-07 - val_loss: 2.4475e-07\n",
      "Epoch 249/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 2.4428e-07 - val_loss: 2.4366e-07\n",
      "Epoch 250/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.4369e-07 - val_loss: 2.4253e-07\n",
      "Epoch 251/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.4253e-07 - val_loss: 2.4137e-07\n",
      "Epoch 252/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.4093e-07 - val_loss: 2.4024e-07\n",
      "Epoch 253/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.4023e-07 - val_loss: 2.3905e-07\n",
      "Epoch 254/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.3862e-07 - val_loss: 2.3794e-07\n",
      "Epoch 255/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.3748e-07 - val_loss: 2.3682e-07\n",
      "Epoch 256/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.3686e-07 - val_loss: 2.3555e-07\n",
      "Epoch 257/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.3561e-07 - val_loss: 2.3427e-07\n",
      "Epoch 258/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.3373e-07 - val_loss: 2.3307e-07\n",
      "Epoch 259/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.3249e-07 - val_loss: 2.3183e-07\n",
      "Epoch 260/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.3193e-07 - val_loss: 2.3051e-07\n",
      "Epoch 261/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.3061e-07 - val_loss: 2.2920e-07\n",
      "Epoch 262/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.2862e-07 - val_loss: 2.2791e-07\n",
      "Epoch 263/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.2733e-07 - val_loss: 2.2660e-07\n",
      "Epoch 264/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.2602e-07 - val_loss: 2.2527e-07\n",
      "Epoch 265/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.2534e-07 - val_loss: 2.2386e-07\n",
      "Epoch 266/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.2392e-07 - val_loss: 2.2244e-07\n",
      "Epoch 267/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.2186e-07 - val_loss: 2.2105e-07\n",
      "Epoch 268/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.2047e-07 - val_loss: 2.1967e-07\n",
      "Epoch 269/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.1907e-07 - val_loss: 2.1827e-07\n",
      "Epoch 270/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.1834e-07 - val_loss: 2.1674e-07\n",
      "Epoch 271/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.1683e-07 - val_loss: 2.1522e-07\n",
      "Epoch 272/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.1531e-07 - val_loss: 2.1370e-07\n",
      "Epoch 273/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.1306e-07 - val_loss: 2.1221e-07\n",
      "Epoch 274/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.1228e-07 - val_loss: 2.1065e-07\n",
      "Epoch 275/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.1071e-07 - val_loss: 2.0907e-07\n",
      "Epoch 276/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.0845e-07 - val_loss: 2.0755e-07\n",
      "Epoch 277/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.0692e-07 - val_loss: 2.0603e-07\n",
      "Epoch 278/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.0610e-07 - val_loss: 2.0438e-07\n",
      "Epoch 279/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.0447e-07 - val_loss: 2.0273e-07\n",
      "Epoch 280/400\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 2.0205e-07 - val_loss: 2.0116e-07\n",
      "Epoch 281/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.0126e-07 - val_loss: 1.9949e-07\n",
      "Epoch 282/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.9958e-07 - val_loss: 1.9782e-07\n",
      "Epoch 283/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.9789e-07 - val_loss: 1.9613e-07\n",
      "Epoch 284/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.9547e-07 - val_loss: 1.9449e-07\n",
      "Epoch 285/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.9383e-07 - val_loss: 1.9288e-07\n",
      "Epoch 286/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1.9218e-07 - val_loss: 1.9125e-07\n",
      "Epoch 287/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 1.9136e-07 - val_loss: 1.8945e-07\n",
      "Epoch 288/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.8868e-07 - val_loss: 1.8777e-07\n",
      "Epoch 289/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.8793e-07 - val_loss: 1.8601e-07\n",
      "Epoch 290/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.8525e-07 - val_loss: 1.8431e-07\n",
      "Epoch 291/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1.8444e-07 - val_loss: 1.8255e-07\n",
      "Epoch 292/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.8263e-07 - val_loss: 1.8075e-07\n",
      "Epoch 293/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.8007e-07 - val_loss: 1.7899e-07\n",
      "Epoch 294/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.7833e-07 - val_loss: 1.7730e-07\n",
      "Epoch 295/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1.7737e-07 - val_loss: 1.7545e-07\n",
      "Epoch 296/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 1.7471e-07 - val_loss: 1.7372e-07\n",
      "Epoch 297/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.7386e-07 - val_loss: 1.7185e-07\n",
      "Epoch 298/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1.7106e-07 - val_loss: 1.7010e-07\n",
      "Epoch 299/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.7027e-07 - val_loss: 1.6829e-07\n",
      "Epoch 300/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.6843e-07 - val_loss: 1.6648e-07\n",
      "Epoch 301/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.6576e-07 - val_loss: 1.6465e-07\n",
      "Epoch 302/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.6396e-07 - val_loss: 1.6290e-07\n",
      "Epoch 303/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.6220e-07 - val_loss: 1.6117e-07\n",
      "Epoch 304/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.6039e-07 - val_loss: 1.5939e-07\n",
      "Epoch 305/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.5853e-07 - val_loss: 1.5756e-07\n",
      "Epoch 306/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.5667e-07 - val_loss: 1.5577e-07\n",
      "Epoch 307/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.5489e-07 - val_loss: 1.5403e-07\n",
      "Epoch 308/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.5318e-07 - val_loss: 1.5227e-07\n",
      "Epoch 309/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.5145e-07 - val_loss: 1.5045e-07\n",
      "Epoch 310/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.4968e-07 - val_loss: 1.4862e-07\n",
      "Epoch 311/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.4788e-07 - val_loss: 1.4682e-07\n",
      "Epoch 312/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.4694e-07 - val_loss: 1.4495e-07\n",
      "Epoch 313/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 1.4506e-07 - val_loss: 1.4309e-07\n",
      "Epoch 314/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.4240e-07 - val_loss: 1.4132e-07\n",
      "Epoch 315/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.4062e-07 - val_loss: 1.3960e-07\n",
      "Epoch 316/400\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 1.3884e-07 - val_loss: 1.3783e-07\n",
      "Epoch 317/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.3700e-07 - val_loss: 1.3604e-07\n",
      "Epoch 318/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 1.3631e-07 - val_loss: 1.3429e-07\n",
      "Epoch 319/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.3454e-07 - val_loss: 1.3261e-07\n",
      "Epoch 320/400\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 1.3189e-07 - val_loss: 1.3069e-07\n",
      "Epoch 321/400\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 1.3007e-07 - val_loss: 1.2897e-07\n",
      "Epoch 322/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.2837e-07 - val_loss: 1.2736e-07\n",
      "Epoch 323/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.2664e-07 - val_loss: 1.2564e-07\n",
      "Epoch 324/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.2478e-07 - val_loss: 1.2384e-07\n",
      "Epoch 325/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.2295e-07 - val_loss: 1.2219e-07\n",
      "Epoch 326/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.2256e-07 - val_loss: 1.2073e-07\n",
      "Epoch 327/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.1998e-07 - val_loss: 1.1888e-07\n",
      "Epoch 328/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1.1824e-07 - val_loss: 1.1708e-07\n",
      "Epoch 329/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.1654e-07 - val_loss: 1.1553e-07\n",
      "Epoch 330/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1.1492e-07 - val_loss: 1.1396e-07\n",
      "Epoch 331/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.1412e-07 - val_loss: 1.1211e-07\n",
      "Epoch 332/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 1.1135e-07 - val_loss: 1.1053e-07\n",
      "Epoch 333/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.0979e-07 - val_loss: 1.0896e-07\n",
      "Epoch 334/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.0825e-07 - val_loss: 1.0738e-07\n",
      "Epoch 335/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.0671e-07 - val_loss: 1.0580e-07\n",
      "Epoch 336/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.0515e-07 - val_loss: 1.0424e-07\n",
      "Epoch 337/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.0441e-07 - val_loss: 1.0266e-07\n",
      "Epoch 338/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.0279e-07 - val_loss: 1.0109e-07\n",
      "Epoch 339/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.0113e-07 - val_loss: 9.9485e-08\n",
      "Epoch 340/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 9.9471e-08 - val_loss: 9.7934e-08\n",
      "Epoch 341/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 9.7535e-08 - val_loss: 9.6663e-08\n",
      "Epoch 342/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 9.6737e-08 - val_loss: 9.5009e-08\n",
      "Epoch 343/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 9.5181e-08 - val_loss: 9.3481e-08\n",
      "Epoch 344/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 9.2869e-08 - val_loss: 9.2077e-08\n",
      "Epoch 345/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 9.1481e-08 - val_loss: 9.0685e-08\n",
      "Epoch 346/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 9.0096e-08 - val_loss: 8.9309e-08\n",
      "Epoch 347/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 8.9504e-08 - val_loss: 8.7936e-08\n",
      "Epoch 348/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 8.7398e-08 - val_loss: 8.6539e-08\n",
      "Epoch 349/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 8.6641e-08 - val_loss: 8.5151e-08\n",
      "Epoch 350/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 8.5216e-08 - val_loss: 8.3773e-08\n",
      "Epoch 351/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 8.3804e-08 - val_loss: 8.2420e-08\n",
      "Epoch 352/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 8.2446e-08 - val_loss: 8.1100e-08\n",
      "Epoch 353/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 8.0727e-08 - val_loss: 7.9961e-08\n",
      "Epoch 354/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 7.9455e-08 - val_loss: 7.8677e-08\n",
      "Epoch 355/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 7.8026e-08 - val_loss: 7.7378e-08\n",
      "Epoch 356/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 7.6734e-08 - val_loss: 7.6265e-08\n",
      "Epoch 357/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 7.5691e-08 - val_loss: 7.5111e-08\n",
      "Epoch 358/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 7.4605e-08 - val_loss: 7.3823e-08\n",
      "Epoch 359/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 7.3403e-08 - val_loss: 7.2625e-08\n",
      "Epoch 360/400\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 7.2711e-08 - val_loss: 7.1405e-08\n",
      "Epoch 361/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 7.1011e-08 - val_loss: 7.0302e-08\n",
      "Epoch 362/400\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 7.0433e-08 - val_loss: 6.9097e-08\n",
      "Epoch 363/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 6.9248e-08 - val_loss: 6.7978e-08\n",
      "Epoch 364/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6.8052e-08 - val_loss: 6.6824e-08\n",
      "Epoch 365/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 6.6531e-08 - val_loss: 6.5812e-08\n",
      "Epoch 366/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 6.5507e-08 - val_loss: 6.4843e-08\n",
      "Epoch 367/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6.4339e-08 - val_loss: 6.3677e-08\n",
      "Epoch 368/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 6.3981e-08 - val_loss: 6.2766e-08\n",
      "Epoch 369/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 6.2299e-08 - val_loss: 6.1709e-08\n",
      "Epoch 370/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 6.1325e-08 - val_loss: 6.0584e-08\n",
      "Epoch 371/400\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 6.0623e-08 - val_loss: 5.9553e-08\n",
      "Epoch 372/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 5.9302e-08 - val_loss: 5.8689e-08\n",
      "Epoch 373/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 5.8344e-08 - val_loss: 5.7724e-08\n",
      "Epoch 374/400\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 5.7917e-08 - val_loss: 5.6712e-08\n",
      "Epoch 375/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 5.6287e-08 - val_loss: 5.5807e-08\n",
      "Epoch 376/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 5.5962e-08 - val_loss: 5.4896e-08\n",
      "Epoch 377/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 5.4885e-08 - val_loss: 5.3869e-08\n",
      "Epoch 378/400\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 5.3735e-08 - val_loss: 5.3186e-08\n",
      "Epoch 379/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 5.2981e-08 - val_loss: 5.2384e-08\n",
      "Epoch 380/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 5.1878e-08 - val_loss: 5.1291e-08\n",
      "Epoch 381/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5.1663e-08 - val_loss: 5.0763e-08\n",
      "Epoch 382/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 5.1047e-08 - val_loss: 5.0005e-08\n",
      "Epoch 383/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 4.9832e-08 - val_loss: 4.8721e-08\n",
      "Epoch 384/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.8552e-08 - val_loss: 4.8119e-08\n",
      "Epoch 385/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 4.8229e-08 - val_loss: 4.7862e-08\n",
      "Epoch 386/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.7402e-08 - val_loss: 4.6512e-08\n",
      "Epoch 387/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 4.5861e-08 - val_loss: 4.5735e-08\n",
      "Epoch 388/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.6363e-08 - val_loss: 4.5832e-08\n",
      "Epoch 389/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 4.5405e-08 - val_loss: 4.4278e-08\n",
      "Epoch 390/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 4.4147e-08 - val_loss: 4.3533e-08\n",
      "Epoch 391/400\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 4.3522e-08 - val_loss: 4.2953e-08\n",
      "Epoch 392/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.2722e-08 - val_loss: 4.2124e-08\n",
      "Epoch 393/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.2292e-08 - val_loss: 4.1299e-08\n",
      "Epoch 394/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 4.1658e-08 - val_loss: 4.0955e-08\n",
      "Epoch 395/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 4.0649e-08 - val_loss: 3.9877e-08\n",
      "Epoch 396/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 3.9793e-08 - val_loss: 3.9354e-08\n",
      "Epoch 397/400\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 3.9369e-08 - val_loss: 3.8626e-08\n",
      "Epoch 398/400\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 3.8707e-08 - val_loss: 3.7872e-08\n",
      "Epoch 399/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 3.7603e-08 - val_loss: 3.7263e-08\n",
      "Epoch 400/400\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 3.6998e-08 - val_loss: 3.6662e-08\n",
      "2/2 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rnn_plus_wLRS_wtCMF_tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>addingProblem.bs=100.ts=100.csv</th>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 rnn_plus_wLRS_wtCMF_tf\n",
       "addingProblem.bs=100.ts=100.csv                   100.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_tf = {}\n",
    "model_name = 'rnn_plus_wLRS_wtCMF_tf'\n",
    "result_tf[model_name]= {}\n",
    "for filename in fileslist:\n",
    "    filepath = os.path.join(path,filename)\n",
    "    result_tf[model_name][filename] = []\n",
    "    print('++', filename)\n",
    "    \n",
    "    logdir = f\"./logs/scalars/{model_name}_{filename}\"\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "    if ISMOORE_DATASETS:\n",
    "        timestep = int(filename.split('.')[4].split('s')[-1])      \n",
    "    else:\n",
    "        timestep = int(filename.split('.')[2].split('=')[-1])\n",
    "        batchsize_10x = int(filename.split('.')[1].split('=')[-1])\n",
    "        hyperparams['batchSize'] = 50 if batchsize_10x%50 == 0 else 20\n",
    "        print(\"+++ Batch Size: \", hyperparams['batchSize'])\n",
    "\n",
    "    with open(filepath, \"r\") as fp:\n",
    "        [noIn, noOut] = [int(x) for x in fp.readline().replace('\\n', '').split(',')]\n",
    "        rdf = np.genfromtxt(fp, delimiter=',')\n",
    "        \n",
    "    print(f\"Step 1: Dividing the training and testing set with ratio 1:1 ({hyperparams['testSize']*100}%).\")\n",
    "    df_val, df_train = train_test_split(rdf,test_size=hyperparams['testSize'])\n",
    "    print(\"+ Training set:   \", df_train.shape, df_val.shape)\n",
    "\n",
    "    x_train, y_train = seperateValues(df_train, noIn, noOut, isMoore=ISMOORE_DATASETS)\n",
    "    x_val, y_val = seperateValues(df_val, noIn, noOut, isMoore=ISMOORE_DATASETS)    \n",
    "    for i in range( x_train.shape[ 0 ] ) :\n",
    "        for j in range( x_train.shape[ 1 ] ) :\n",
    "            for k in range( x_train.shape[ 2 ] ) :\n",
    "                x_train[ i, j, k ] = fromBit( x_train[ i, j, k ] )\n",
    "    for i in range( y_train.shape[ 0 ] ) :\n",
    "        for j in range( y_train.shape[ 1 ] ) :\n",
    "            y_train[ i, j ] = fromBit( y_train[ i, j ] )\n",
    "    for i in range( x_val.shape[ 0 ] ) :\n",
    "        for j in range( x_val.shape[ 1 ] ) :\n",
    "            for k in range( x_val.shape[ 2 ] ) :\n",
    "                x_val[ i, j, k ] = fromBit( x_val[ i, j, k ] )\n",
    "    for i in range( y_val.shape[ 0 ] ) :\n",
    "        for j in range( y_val.shape[ 1 ] ) :\n",
    "            y_val[ i, j ] = fromBit( y_val[ i, j ] )\n",
    "    print(\"+ Training set:   \", x_train.shape, y_train.shape, x_train.dtype)\n",
    "    print(\"+ Validating set: \", x_val.shape, y_val.shape, x_val.dtype)\n",
    "    \n",
    "    for run_count in range(1):\n",
    "        model = models(modeltype=\"RNN_plus\", timestep=timestep, noInput=noIn, noOutput=noOut, batchSize=int(hyperparams['batchSize']), isLRS=True, isCMF=False).chooseModels()\n",
    "        model_history = model.fit(\n",
    "                    x_train, y_train,\n",
    "                    batch_size=int(hyperparams['batchSize']),\n",
    "                    verbose=0, # Suppress chatty output; use Tensorboard instead\n",
    "                    epochs=int(hyperparams['numTrainingSteps']/(x_train.shape[0])),\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    shuffle=True,\n",
    "                    use_multiprocessing=False,\n",
    "                    callbacks=[tensorboard_callback]\n",
    "                )\n",
    "        y_pred = model.predict(x_val, verbose=1, batch_size=int(hyperparams['batchSize']))\n",
    "        result_tf[model_name][filename].append(round(customMetricfn(y_val, y_pred), 5)*100)\n",
    "\n",
    "# results_dir_o = \"./results/merge_results_5bigdatasets.csv\"\n",
    "# readfile = pd.read_csv(results_dir_o, index_col='dataset')\n",
    "# readfile.index.name = None\n",
    "\n",
    "for k, v in result_tf[model_name].items():\n",
    "    result_tf[model_name][k] = round(sum(result_tf[model_name][k]) / len(result_tf[model_name][k]), 2)\n",
    "\n",
    "model_result = pd.DataFrame.from_dict(result_tf[model_name], orient='index', columns=[model_name])\n",
    "display(model_result)\n",
    "# pd.concat([model_result, readfile], axis=1).to_csv(results_dir_o, index=True, index_label='dataset', mode='w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95275aa-ca37-483a-afcf-b3b6634379b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "master_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
